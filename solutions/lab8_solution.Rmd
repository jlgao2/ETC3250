---
title: "Lab 8"
subtitle: "Monash University, Econ & Bus Stat, ETC3250/5250"
author: "prepared by Professor Di Cook"
date: "Materials for Week 8"
output:
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  eval = TRUE,
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 6,
  fig.align = "center",
  cache = FALSE
)
```

# Objective

The objectives for this week are:

- Understand the the process of boosting to improve model accuracy
- Learn about fitting a support vector machine model 
- Examine the differences between results from different models

# Class discussion 

This is a diagram explaining boosting. The three tree models in the top row are combined to give the boosted model in box 4. Come up with a some words and sentences, together, to explain the process. 

![](images/boosting.png)

**Compare with the explanation at https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/**

How would a single tree with multiple splits fit this data? What is different about the two approaches?

**It might be almost the same. The first two splits might be the same as box 1, 2. The third split would be only on the subset in the middle. The difference is that with boosting all observations are used each split, but weighted differently. You could think of the single tree as having weights too, either 0 or 1.**

# Theory

Fill in the steps to go from the first line to the last

$$
\begin{align*}
\mathcal{K}(\mathbf{x}, \mathbf{y}) & = (1 + \langle \mathbf{x}, \mathbf{y}\rangle) ^2 \\
                                    & = \left(1 + \sum_{j = 1}^2 x_jy_j \right) ^2 \\
                                    & = (1 + x_1y_1 + x_2y_2)^2 \\
                                    & = (1 + x_1^2y_1^2 + x_2^2y_2^2 + 2x_1y_1 + 2x_2y_2 + 2x_1x_2y_1y_2) \\
                                    & = (1, x_1^2, x_2^2, \sqrt2x_1, \sqrt2x_2, \sqrt2x_1x_2)^T (1, y_1^2, y_2^2, \sqrt2y_1, \sqrt2y_2, \sqrt2y_1y_2)\\
                                    & = \langle \psi(\mathbf{x}), \psi(\mathbf{y}) \rangle
\end{align*}
$$

# Practice

## 1. 

Fit the linear SVM to olive oils, using a training split of 2/3, using only regions 2, 3,  and  the predictors linoleic and arachidic. 

```{r}
library(tidyverse)
library(e1071)
olive <- read_csv("http://www.ggobi.org/book/data/olive.csv") %>%
  rename(name=X1) %>%
  dplyr::select(-name, -area) %>%
  filter(region != 1) %>%
  mutate(region = factor(region)) 
olive_sub <- olive %>%
  select(region, linoleic, arachidic)
library(caret)
set.seed(20200501)
tr_indx <- createDataPartition(olive_sub$region, times=10, p=0.67)
olive_tr <- olive_sub[tr_indx$Resample03,]
olive_ts <- olive_sub[-tr_indx$Resample03,]
olive_svm <- svm(region~linoleic+arachidic,  data=olive_tr, kernel="linear")
```

a. Report the training and test error, 

```{r}
1-confusionMatrix(olive_tr$region, predict(olive_svm, newdata=olive_tr, type="class"))$overall[1]
1-confusionMatrix(olive_ts$region, predict(olive_svm, newdata=olive_ts, type="class"))$overall[1]
```

b. list the support vectors, 

```{r}
cbind(olive_svm$index, olive_svm$SV)
```

c. the coefficients for the support vectors and 

```{r}
olive_svm$coefs
```

d. the equation for the separating hyperplane, and

```{r}
beta <- t(olive_svm$coefs)%*%olive_svm$SV
```

$$2.20\times\text{linoleic}+0.643\times\text{arachidic}+0.889 = 0$$

e. make a plot of the boundary.
```{r}
olive_grid <- expand_grid(linoleic=seq(min(olive_sub$linoleic), max(olive_sub$linoleic), 10), 
                       arachidic=seq(min(olive_sub$arachidic), max(olive_sub$arachidic), 1))
olive_grid$region <- predict(olive_svm, olive_grid)
ggplot() +
  geom_point(data=olive_grid, aes(x=linoleic, y=arachidic, colour=region), alpha=0.1) +
  geom_point(data=olive_sub, aes(x=linoleic, y=arachidic, colour=region, shape=region), size=2) +
  scale_colour_brewer("region", palette="Dark2") +
  theme(aspect.ratio=1)
```
f. Write a paragraph explaining how this model fit differs from the tree, and random forests fit the data in the last lab.

**The SVM model fits similarly to othe tree model with the variable linoarch. It finds the gap that uses a combination of linoleic and arachidic.**

## 2. 
Fit the SVM again to the full set of variables. Generate the predictions from this model for your gridded data, and  plot them for linoleic and arachidic acid. You will need to set some fixed value for the other variables, say the mean, so that the gridded data has all variables. Explain how the boundary changes, if is does.

```{r}
olive_tr <- olive[tr_indx$Resample03,]
olive_ts <- olive[-tr_indx$Resample03,]
olive_svm <- svm(region~.,  data=olive_tr, kernel="linear")
olive_grid$palmitic <- mean(olive_tr$palmitic)
olive_grid$palmitoleic <- mean(olive_tr$palmitoleic)
olive_grid$stearic <- mean(olive_tr$stearic)
olive_grid$oleic  <- mean(olive_tr$oleic)
olive_grid$linolenic <- mean(olive_tr$linolenic)
olive_grid$eicosenoic <- mean(olive_tr$eicosenoic)
olive_grid$region <- predict(olive_svm, olive_grid)
ggplot() +
  geom_point(data=olive_grid, aes(x=linoleic, y=arachidic, colour=region), alpha=0.1) +
  geom_point(data=olive_sub, aes(x=linoleic, y=arachidic, colour=region, shape=region), size=2) +
  scale_colour_brewer("region", palette="Dark2") +
  theme(aspect.ratio=1)
```

**It does move. This indicates that some of the oother variables are being used to build the boundary. The accuracy may be as good, even though it doesn't look so good in this slice. If we examined the coeficients for the   separating hyperplane, my guess is that it is oleic acid that is influencing the boundary change.**

##  3. 

This last question revisits the paintings problem, to see how random forests compares with a boosted tree model, on really tough problem. 

The purpose is to automatically analyse the happy paintings by Bob Ross. This was the subject of the [538 post](http://fivethirtyeight.com/features/a-statistical-analysis-of-the-work-of-bob-ross/), "A Statistical Analysis of the Work of Bob Ross".

We have taken the painting images from the [sales site](http://www.saleoilpaintings.com/paintings/bob-ross/bob-ross-sale-3_1.html), read the images into R, and resized them all to be 20 by 20 pixels. Each painting has been classified into one of 8 classes based on the title of the painting. This is the data that you will work with.

It is provided in wide and long form. Long form is good for making pictures of the original painting, and the wide form is what you will need to use for fitting the classification models. In wide form, each row corresponds to one painting, and the rgb color values at each pixel are in each column. With a $20\times20$ image, this leads to $400\times3=1200$ columns.

Here are three of the original paintings in the collection, labelled as "scene", "water", "flowers":

![bobross5](images/bobross5.jpg)
![bobross140](images/bobross140.jpg)
![bobross167](images/bobross167.jpg)

```{r fig.width=8, fig.height=2.5, eval=TRUE}
library(gridExtra)
paintings <- read_csv("data/paintings-train.csv")
paintings_long <- read_csv("data/paintings-long-train.csv")
paintings_test <- read_csv("data/paintings-test.csv")
df <- filter(paintings_long, id == 5)
p1 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
df <- filter(paintings_long, id == 140)
p2 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
df <- filter(paintings_long, id == 167)
p3 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
grid.arrange(p1, p2, p3, ncol=3)
```


a. Build a random forest for the training data, for two classes, `flowers` and `water`. Predict the class of test set, report the error.

```{r}
library(randomForest)
p_sub <- paintings %>% 
  filter(class %in% c("flowers", "water")) %>% 
  arrange(class) %>%
  mutate(class = factor(class))
p_rf <- randomForest(class~., data=p_sub[,-c(1,2)], ntree=10000,
                     importance=TRUE)
p_rf

# Predict test
p_test_sub <- paintings_test %>% 
  filter(class %in% c("flowers", "water")) %>% 
  arrange(class) %>%
  mutate(class = factor(class))

addmargins(table(predict(p_rf, newdata=p_test_sub, type="class"), p_test_sub$class))
```

b.  Read the description of the XGBoost technique at https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/, or other sources. Tune the model fit to determine how many iterations to make. Then fit the model, using the parameter set provided. Fit the xgboost model to the paintings data. Compute the error for the test set, and describe the difference with the results from the  random forest.

```{r results='hide'}
library(xgboost)
p_tr_xg <- xgb.DMatrix(data = as.matrix(p_sub[,-c(1:3)]), 
                       label = ifelse(p_sub[,3]=="water", "1", "0"))
p_ts_xg <- xgb.DMatrix(data = as.matrix(p_test_sub[,-c(1:3)]), 
                       label =  ifelse(p_test_sub[,3]=="water", "1", "0"))

params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

xgbcv <- xgb.cv(params = params, data = p_tr_xg, nrounds = 100, nfold = 5,
                showsd = T, stratified = T, maximize = F)

p_xgb <- xgb.train(params = params, data = p_tr_xg, nrounds = 10, 
                   watchlist = list(eval=p_ts_xg, train=p_tr_xg))
```

```{r}
xgbpred <- predict(p_xgb, p_ts_xg)
xgbpred <- ifelse(xgbpred > 0.5, 1, 0)
addmargins(table(xgbpred, p_test_sub$class))
```
