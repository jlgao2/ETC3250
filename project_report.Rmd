---
title: 'ETC3250/5350: Project report'
author: "CHANGE ME: Add your team members names here, and team name"
date: "Jun 7, 2020"
output: 
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10,
  fig.height = 5, 
  out.width = "100%",
  fig.retina = 3)
```

## `r emo::ji("llama")` Introduction

Sentence about the data

- what are the variables, 
- what are the observations, and how many

INCLUDE YOUR VERSION OF A SKETCH OF SAMPLES, eg

```{r fig.height=2}
load("data/sketches_train.rda")
library(ggpubr)
library(tidyverse)
sketch1 <- sketches %>% 
	group_by(word) %>%
	sample_n(1) %>%
	ungroup() %>%
	pivot_longer(cols = contains("V"), names_to = "pixel", values_to = "grey") %>%
	mutate(pixel = as.numeric(sub("V", "", pixel))) %>%
	mutate(x=(pixel-1)%%28+1, y = -(floor((pixel-1)/28)+1))
ggplot(sketch1, aes(x=x, y=y, fill=grey)) + geom_tile() + 
	scale_fill_distiller("", palette="Greys", direction=1) + 
	facet_wrap(~word, ncol=6) + theme_transparent() + 
	theme(aspect.ratio=1, legend.position="none")
```

## `r emo::ji("wrench")` Methodology  

The data is structurally similar to the popular MNIST[7] and Fashion MNIST[2] datasets that are a common benchmark and starting point for machine learning research and learning. To achieve the best classification performance, a deep residual neural network with preactivations and Fmix augmentation is currently the benchmark model for this classification task[4], a similar approach is ideally used as a starting point. However reproducing the entirety of the paper is beyond the scope and timeframe of this project and some simplifications must be made. 

With the resource and learning curve of cutting edge deep learning technologies in mind, the authors used off the shelf network architectures that are popularly being regarded as being robust and having high predictive performance. Such networks include Residual Networks[5] (Resnets), both 18 and 34 layers deep, this has the following architecture. 

![](https://miro.medium.com/max/512/1*kBlZtheCjJiA3F1e0IurCw.png)

Due to the small size of the data and the dataset in general, training time is not a severe limitation. Therefore a Resnet-34 architecture was used for majority of training. 

Due to related work in George's FYP, it is also examined whether additional classification performance can be obtained by up-sampling the dataset using an Enhances Super-Resolution Generative Adversarial Network (ESRGAN) [11] network to improve signal to noise ratio and allow for great flexibility in data augmentation, in the limited conditions considered, this appears to greatly improve classification performance by up to 2x.

The fastai framework[6] for pytorch[1] was used to eliminate boilerplate engineering code and gain useful default parameters and transformations. Notably it automates the application of LN Smith's one cycle tuning policy[2].  

![Learning rate finder](https://raw.githubusercontent.com/jlgao2/ETC3250/master/images/learning_rate_finder.png)

The learning rate range used for the one cycle training policy is found by running a range of learning rates over one epoch and finding a section where it descends towards the absolute minima.

The default transformations used for data augmentation are. The network was used with 

    get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, 

Furthermore there are some samples that are should not attributed to a class/contribute any information that the network can use. Therefore we have also removed them, which resulted in a minor improvement in performance as well. 

![Bad images](https://raw.githubusercontent.com/jlgao2/ETC3250/master/images/Discarded_images.png)

Brief summary of models studied


| Model                                   | Error_Rate | Kaggle Accuracy |
|-----------------------------------------|------------|-----------------|
| Simplenet                               | 0.088      | 0.916           |
| Resnet-34 Baseline with Mixup           | 0.130      |                 |
| Resnet-34 with Mixup and Up-sampled Data| 0.062      | 9.350           |
| Resnet-34 with Cleaned Up-sampled Data  | 0.046      | 9.433           |
| Resnet-34 with Up-sampled Data          | 0.051      |                 |
| Resnet-18 ""                            | 0.053      | 9.450           |
 

## `r emo::ji("seedling")` Results and Discussion

This includes for example graphs and tables, as well as a discussion of the results. You should summarise your training error, the important variables. Include at least one plot that is important, ideally of the important variables, or of observations that are consistently misclassified. It could be good to have one interesting fact about the data. 

![Progress when training last cycle](https://raw.githubusercontent.com/jlgao2/ETC3250/master/images/accuracy_epoch.png)

![Confusion matrix](https://raw.githubusercontent.com/jlgao2/ETC3250/master/images/confusion_matrix.png)
![Most misclassified data](https://raw.githubusercontent.com/jlgao2/ETC3250/master/images/top_losses.png)



```{r}
library(gridExtra)
library(ggpubr)
p1 <- ggplot(data=sketches, aes(x=word, y=V71, fill=word)) + geom_violin() + theme_minimal() + theme(legend.position = "none")
p2 <- ggplot(data=sketches, aes(x=word, y=V99, fill=word)) + geom_violin() + theme_minimal() + theme(legend.position = "none")
grid.arrange(p1, p2, ncol=2)
```

## `r emo::ji("moon_cake")` Conclusion

Short paragraph about what you have learned from the model, getting to your best model, and about the data . 

##  `r emo::ji("apple")` References

[1]A. Paszke et al., ‘PyTorch: An imperative style, high-performance deep learning library’, in Advances in Neural Information Processing Systems, 2019, pp. 8024–8035.
[2]L. N. Smith and N. Topin, ‘Super-convergence: Very fast training of neural networks using large learning rates’, in Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications, 2019, vol. 11006, p. 1100612.
[3]S. Chetlur et al., ‘cudnn: Efficient primitives for deep learning’, arXiv preprint arXiv:1410.0759, 2014.
[4]E. Harris, A. Marcu, M. Painter, M. Niranjan, A. Prügel-Bennett, and J. Hare, ‘Understanding and Enhancing Mixed Sample Data Augmentation’, arXiv preprint arXiv:2002.12047, 2020.
[5]K. He, X. Zhang, S. Ren, and J. Sun, ‘Deep Residual Learning for Image Recognition’, arXiv:1512.03385 [cs], Dec. 2015, Accessed: Jun. 02, 2020. [Online]. Available: http://arxiv.org/abs/1512.03385.
[6]J. Howard and S. Gugger, ‘Fastai: A layered API for deep learning’, Information, vol. 11, no. 2, p. 108, 2020.
[7]Y. LeCun, C. Cortes, and C. J. Burges, ‘The MNIST database of handwritten digits, 1998’, URL http://yann. lecun. com/exdb/mnist, vol. 10, p. 34, 1998.
[8]T. E. Oliphant, ‘Python for scientific computing’, Computing in Science & Engineering, vol. 9, no. 3, pp. 10–20, 2007.
[9]L. N. Smith, ‘A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay’, arXiv preprint arXiv:1803.09820, 2018.
[10]S. van der Walt, S. C. Colbert, and G. Varoquaux, ‘The NumPy array: a structure for efficient numerical computation’, Computing in Science & Engineering, vol. 13, no. 2, pp. 22–30, 2011.
[11]X. Wang et al., ‘ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks’, arXiv:1809.00219 [cs], Sep. 2018, Accessed: Jun. 02, 2020. [Online]. Available: http://arxiv.org/abs/1809.00219.
[12]H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, ‘mixup: Beyond empirical risk minimization’, arXiv preprint arXiv:1710.09412, 2017.


**This section does NOT count in the 5 pages**

## `r emo::ji("cricket")` Appendix

Anything else you  would like to include but that are not the most important things. 

**This section does NOT count in the 5 pages**
