---
title: "ETC3250 ASSIGNMENT 2"
author: "Tarushi Hondhe-Munige"
date: "14/04/2020"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE)
library(tidyverse)
library(lubridate)
```

## Exercises

### ***1. (5pts) This question is about the normal distribution, and how it relates to the classification rule provided by quadratic discriminant analysis.***

a. Write down the density function for a univariate normal distribution ($p=1$), with mean $\mu_k$ and variance $\sigma_k$. 

$$f(x)=\frac{1}{\sigma_k\sqrt2\pi}e^{\frac{-1}{2}(\frac{x-\mu_k}{\sigma_k})^2}$$

b. Show that the quadratic discriminant rule for two groups ($K=2$), $\pi_1=\pi_2$ is equal to:
*Assign a new observation $x_0$ to group 1 if*
$$-\frac12 \left( \frac{1}{\sigma_1^2}- \frac{1}{\sigma_2^2} \right) {x_0^2} + \left(\frac{\mu_1}{\sigma_1^2}-\frac{\mu_2}{\sigma_2^2}\right)x_0 -\frac12 \left(\frac{\mu_1^2}{\sigma_1^2}-  \frac{\mu_2^2}{\sigma_2^2} \right)  -\log{\sigma_1}+\log{\sigma_2}>0$$

c. Suppose $\mu_1=4, \mu_2=-5, \sigma_1=0.5, \sigma_2=5$ simulate a set of 50 observations from each population. Make a plot of the population model, and add these samples as a rug plot on the horizontal axis. (See the lecture notes for a similar plot and code for linear discriminant analysis.)

```{r fig.width=6, fig.height=4, out.width="80%"}
library(tidyverse)
set.seed("24042020")
n <- 100; n1 <- 50
m1 <- 4
m2 <- -5
s1 <- 0.5
s2 <- 5
x <- c(rnorm(n1, m1, s1), rnorm(n-n1, m2, s2))
y <- c(rep(1, n1), rep(2, n-n1))
df <- tibble(x, y)

x <- seq(-20, 20, 0.2)
dx <- c(dnorm(x, m1, s1), dnorm(x, m2, s2))
y <- factor(c(rep(0, length(x)), rep(1, length(x))))
df_pop <- tibble(x=c(x,x), dx, y)
p <- ggplot() + 
  geom_rug(data=df, aes(x=x, y=0, colour=factor(y)), alpha=0.7) +
  geom_line(data=df_pop, aes(x=x, y=dx, colour=y)) + 
  scale_color_brewer("", palette="Dark2") +
  xlab("x")  + ylab("density")
p
```

d. Write down the rule using these parameter values, and sketch the boundary corresponding to the rule on the previous plot.

```{r}
a = -0.5*((1/s1^2)-(1/s2^2))
b = m1/s1^2 - m2/s2^2
c = -0.5*(m1^2/s1^2-m2^2/s2^2)
x_pve = (-b + sqrt(b^2 - 4*a*c))/(2*a)
x_nve = (-b - sqrt(b^2 - 4*a*c))/(2*a)
p + geom_vline(xintercept=x_pve, linetype=2)
```

e. If instead you had made a mistake and assumed that the two variances were equal, this would have produced a linear discrimant rule. Mark this boundary on the previous plot. Explain why and how this differs from result of the QDA rule.

```{r}
p + geom_vline(xintercept=x_pve, linetype=2) +
  geom_vline(xintercept=(m1+m2)/2, linetype=3, colour="red")
```

- Linear discriminan rule only solves if variance-covariance is equal.
- As seen above, the QDA includes the different variances of each of the categories, thus better than if the LDA was implemented.


### ***2. (4pts)In this question you are going to practice conducting bootstrap to obtain confidence intervals for a reasonably complicated yet simple analysis.***

*A significant gender gap in maths performance in favour of male students has returned, despite closing in 2015* [Natassia Chrysanthos, Sydney Morning Herald](https://www.smh.com.au/national/nsw/urgent-need-to-address-maths-performance-as-nsw-slumps-in-international-test-20191203-p53ge2.html)

Last December, the 2018 [OECD PISA results](http://www.oecd.org/pisa/data/) were released. These are standardised test scores in math, reading and science, of 15 year olds across the globe. It led to a flurry of articles in the news about slipping standards of Australian students. If you also browsed the news of other countries (including New Zealand, Indonesia, Finland), you would find that many had similarly woeful stories. The above headline focuses on the math gap. To explore this, we will compute bootstrap confidence intervals for the difference between weighted averages for boys and girls in each country. The data is from the 2015 results.  

The code block below will compute the difference between weighted averages for boys and girls in each country. A weighted average is often used with survey data, to reflect how the sampling was done relative to the population characteristics. The weighted average will typically better reflect the population mean. 

```{r gender_means}
library(tidyverse)
library(ISOcodes)
data("ISO_3166_1")

# Load data
load("data/pisa_scores.rda")

# The country information will be used to join the data with map data 
# and the ISOcodes package provides information about codes and country
scores <- scores %>% 
  mutate(CNT=recode(CNT, "QES"="ESP", "QCH"="CHN", "QAR"="ARG", "TAP"="TWN")) %>%
  filter(CNT != "QUC") %>%
  filter(CNT != "QUD") %>%
  filter(CNT != "QUE") %>%
  mutate(gender=factor(gender, levels=c(1,2), labels=c("female","male")))
score_gap <- scores %>% 
  group_by(CNT, gender) %>%
  summarise(math=weighted.mean(math, w=w, na.rm=T),
            reading=weighted.mean(reading, w=w, na.rm=T),
            science=weighted.mean(science, w=w, na.rm=T)) %>%
  pivot_longer(cols=c(math, reading, science), names_to="test", values_to="score") %>%
  pivot_wider(names_from=gender, values_from=score) %>%
  mutate(gap = male - female) %>%
  pivot_wider(id_cols=CNT, names_from=test, values_from=gap)
```

This block of code will compute 90% bootstrap confidence intervals for the weighted mean difference. 

```{r}
library(boot)
# Compute confidence intervals
cifn <- function(d, i) {
  x <- d[i,]
  ci <- weighted.mean(x$math[x$gender == "female"], w=x$w[x$gender == "female"], na.rm=T)-
                     weighted.mean(x$math[x$gender == "male"], w=x$w[x$gender == "male"], na.rm=T)
  ci
}
bootfn <- function(d) {
  r <- boot(d, statistic=cifn, R=100)
  l <- sort(r$t)[5]
  u <- sort(r$t)[95]
  ci <- c(l, u)
  return(ci)
}
#student2012.sub.summary.gap.boot <- ddply(student2012.sub, .(CNT), bootfn)
score_gap_boot <- scores %>% 
  split(.$CNT) %>% purrr::map(bootfn) %>% as_tibble() %>%
  pivot_longer(cols=everything(), names_to ="CNT", values_to ="bounds") %>%
  arrange(CNT) %>%
  mutate(bound = rep(c("ml","mu"), length(unique(scores$CNT)))) %>%
  pivot_wider(names_from = bound, values_from = bound)
score_gap <- score_gap %>% 
  left_join(score_gap_boot, by ="CNT")
```

This block of code will add country names, and make dotplots with confidence intervals for the math gap for each country. 
```{r gap_dots, fig.height=8, fig.width=6, out.width="60%"}
score_gap <- score_gap %>%
   left_join(ISO_3166_1[,c("Alpha_3", "Name")], by=c("CNT"="Alpha_3")) %>%
  rename(name = Name)
 score_gap$name[score_gap$CNT == "KSV"] <- "Kosovo"

library(forcats)
score_gap <- score_gap %>% 
  mutate(name = recode(name, "Czechia"="Czech Republic",
                       "Korea, Republic of"="South Korea",
                       "Macedonia, Republic of"="Macedonia",
                       "Moldova, Republic of"="Moldova",
                       "Russian Federation"="Russia",
                       "Taiwan, Province of China"="Taiwan",
                       "Trinidad and Tobago"="Trinidad",
                       "United States"="USA",
                       "United Kingdom"="UK",
                       "Viet Nam"="Vietnam")) 


# ggplot(data=score_gap, aes(x=fct_reorder(name, math), y=math)) +
#   geom_hline(yintercept=0, colour="red") +
#   geom_point() + 
#   geom_errorbar(aes(ymin=ml, ymax=mu), width=0) +
#   coord_flip() + 
#   xlab("") + ylab("Gender Gap") + ylim(c(-35, 35))
```

```{r}
summary(score_gap)
```

Write a paragraph explaining what you learn about the math gap across the countries tested in 2015. 

-  

### ***3. (9pts)A cross-rate is an exchange rate between two currencies computed by reference to a third currency, usually the US dollar. The data file `rates_Nov19_Mar20.csv` was extracted from https://openexchangerates.org using the code below (my API key has been hidden from you):***

```{r eval=FALSE}
# WARNING: YOU DON'T NEED TO RUN THIS CODE!!!!!
library(jsonlite)
library(lubridate)
library(tidyverse)
ru <- NULL
dt <- ymd("2019-11-01")
dt_end <- ymd("2020-03-31")
for (i in 1:151) {
  cat(i,"\n")
  url <- paste("http://openexchangerates.org/api/historical/",dt,".json?app_id=XXX", sep="")
  x <- fromJSON(url)
  x <- x$rates
  if (length(x) == 171)
    x <- x[-c(164,166)]
  ru <- rbind(ru, data.frame(date=dt, x))
  dt <- dt + days(1)
}
rownames(ru) <- ru$date
write_csv(ru, path="data/rates_new.csv")
```

a. (1)What's the data? Make a plot of the Australian dollar against date. Explain how the Australian dollar has changed relative to the US dollar over the 5 month period.

*Over the 5 month period the Australian dollar has weakened against the US dollar, with a big  decline in mid-March as the coronavirus impact affected the world.*

```{r}
library(tidyverse)
rates <- read_csv("rates_Nov19_Mar20.csv")
ggplot(rates, aes(x = ymd(date), y=AUD)) + 
  geom_line()
```

b. (1)You are going to work with these currencies: AUD, CAD, CHF, CNY, EUR, GBP, INR, JPY, KRW, MXN, NZD, RUB, SEK, SGD, ZAR. List the names of the countries and currency name that these codes refer to. Secondary question: why is the USD a constant 1 in this data. 

*The US is the base rate, against which all other currencies are compared.*

- AUD: Australia - Australian Dollar
- CAD: Canada - Canadian Dollar
- CHF: The Swiss Confederation -Swiss Franc
- CNY: The People's Republic of China - Ren Min Bi
- EUR: The European Union - Euro  
- GBP: United Kingdom - Pound Sterling
- INR: Republic of India - Indian Rupee 
- JPY: Japan - Japanese Yen
- KRW: Republic of Korea - Korean Won
- MXN: Mexico - Mexican Peso
- NZD: New Zealand - New Zealand Dollar 
- RUB: Russia - Russian Ruble
- SEK: Sweden -  Swedish Krona
- SGD: Singapore - Singapore Dollar
- ZAR: South Africa - South African Rand

c. (2)The goal of the principal component analysis is to examine the relative movement of this subset of currencies, especially since coronavirus emerged until the end of March. PCA is used to summarise the volatility (variance) in the currencies, relative to each other. To do this you need to: 

    - Standardise all the currencies, individually. The resulting values will have a mean 0 and standard deviation equal to 1.
    - Flip the sigm so that high means the currency strengthened against the USD, and low means that it weakened. Its easier to explain trends, if you don't need to talk with double-negatives.
    - Make a plot of all the currencies to check the result.

```{r}
library(viridisLite)
library(plotly)
rates_sub <- rates %>%
  select(c(date, USD, AUD, CAD, CHF, CNY, EUR, GBP, INR, JPY, KRW, MXN, NZD, RUB, SEK, SGD, ZAR)) %>%
  mutate_if(is.numeric, function(x) -1*(x-mean(x))/sd(x))
  rates_sub_long <- rates_sub %>% 
  pivot_longer(cols = -date, names_to = "currency", values_to = "std_r8s") 
  ggplot(rates_sub_long, aes(x=date, y=std_r8s, colour=currency)) + geom_line() +
  scale_colour_viridis_d("")
  ggplotly() 
  
# ggplotly() Make an interactive plot to browse the currencies
```

d. (5)Conduct a principal component analysis on the subset of currencies. You need to work from a wide format of the data, where dates are in the columns, and currencies are in the rows. Normally, PCA operate on standardised variables but for this data, you need to NOT standardise each date. Think about why this is best.

```{r}
library(ggrepel)
rates_sub_wide <- rates_sub_long %>%
  pivot_wider(id_cols = -date, names_from = date, values_from = std_r8s)
  rates_pca <- prcomp(rates_sub_wide[2:16,2:153], scale=FALSE)
  
screeplot(rates_pca, type="l")
  summary(rates_pca)
  rates_pca$x %>% 
  as_tibble() %>% 
  mutate(currency = as_vector(rates_sub_wide[2:16,1])) %>%
  ggplot(aes(x = PC1, y = PC2)) + 
   geom_point() +
   geom_text(aes(x = PC1, y = PC2, label = currency)) + 
   theme(aspect.ratio=1)
  
rates_pc_loadings <- as_tibble(rates_pca$rotation) %>%
  mutate(date = colnames(rates_sub_wide[2:153]), 
         index = 1:nrow(rates_pca$rotation),
         ymin=rep(0, nrow(rates_pca$rotation)))

ggplot(rates_pc_loadings) + 
  geom_hline(yintercept=c(1/sqrt(150),
                          -1/sqrt(150)), colour="red") + 
  geom_linerange(aes(x=index, ymin=0, ymax=PC1)) +
  geom_point(aes(x=index, y=PC1))
ggplot(rates_pc_loadings) + 
  geom_hline(yintercept=c(1/sqrt(150),
                          -1/sqrt(150)), colour="red") + 
  geom_linerange(aes(x=index, ymin=0, ymax=PC2)) +
  geom_point(aes(x=index, y=PC2))
```

*Why is this data considered to be high-dimensional?*:

- Data sets containing more features than observations are often referred to as high-dimensional (An Introduction to Statistical Learning with Applications in R, 2013, p. 239). In this case, the data comprises of dates and currencies in wide form. As there are more dates than there are currencies in the dataset it is considered high-dimensional.

*Make a scree plot to summarise the variance explained by cumulative principal components. How much of the total variation do two PCs explain?*:

- 

*Plot the first two principal components. Write a summary of what you learn about the similarity and difference between the currencies.*:

- Overall, most of the currencies were similar when PC1 and PC2 were plotted, apart from CNY, CHF, JPY and EUR. As seen in the plot, JPY and EUR showcase a similar nature for both principal components, with a high postive PC2, whereas both CNY and CHF display a positive PC1.

*Plot the loadings for PC1. Add a base line set at $1/\sqrt{15}$. Why use this as a guide? What time frame generated a big movement (or divergence) in the currencies? Which currencies strengthened relative to the USD in that period? What happened to the Australian dollar?*: 

- The use of the baseline acts as a general average for the loadings for PC1. 
- As seen in the plot, divergence in the currencies occurs at between 115 to 120 index, which links to the time frame of the end of February. 
- The currencies that strengthened relative to the USD were EUR, JPY, CHF, and CNY (noticebly the currencies that showcased a different pattern to the general cluster in the PC1 plot).
- The AUD decreased in relation to the USD during this time period, which is further seen with its negative PC1 value.

*Do the same analysis for PC2. What time frame was there another movement of currencies? Which currencies primarily strengthened, and which weakened during this period?*:

- As seen in the plot for PC2, divergence in the currencies occurs at between 65 to 70 index, however it begins to fluctuate at around the 125 index. 
- Currencies that were primarily weakened were CNY and CHF, with negative PC2, while in comparison EUR and JPY have positive PC2.

*Finish with a paragraph summarising what variability the principal components analysis is summarising. What dimension reduction is being done?*:

- The variability the principal components analysis is summarising the variance for each currency between each othe dates. The dimension reduction that is being done in this case is reducing the date variables to principal components. 


### ***4. (2pts)What's wrong with the following statement?***

**Principle component analysis is a dimension reduction technique**.

- Principal component analysis is a method of unsupervised learning, in which it produces low-dimensional representation of a dataset and finds a sequence of linear combinations of particular variables that have the highest variance and are uncorrelated. 
