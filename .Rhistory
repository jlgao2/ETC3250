trainIndex <- createDataPartition(sketches, times=1, list=F, p=0.8, groups=2)
??createDataPartition
# Load Libraries
library(keras)
library(tidyverse)
library(caret)
# Load data
load("data/sketches_train.rda")
load("data/sketches_test.rda")
# Data Preparation
batch_size <- 128
num_classes <- 10
epochs <- 12
# Input image dimensions
img_rows <- 28
img_cols <- 28
trainIndex <- createDataPartition(sketches, times=1, list=F, p=0.8, groups=2)
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
View(trainIndex)
x_train <- sketches[trainIndex, 1:784]
View(x_train)
y_train <- sketches[trainIndex, 785]
View(y_train)
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- sketches[trainIndex, 1:784]
y_train <- sketches[trainIndex, 785]
x_test  <- sketches[-trainIndex, 1:784]
y_test <- sketches[-trainIndex, 785]
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- sketches[trainIndex, 785]
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- sketches[-trainIndex, 785]
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
y_test <- to_categorical(y_test, num_classes)
?to_categorical
y_train <- as.numeric(sketches[trainIndex, 785])
y_train <- as.numeric(as.character(sketches[trainIndex, 785]))
y_train <- data.matrix(sketches[trainIndex, 785])
View(y_train)
y_test <- data.matrix(sketches[-trainIndex, 785])
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785])
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785])
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
num_classes <- 6
epochs <- 12
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785])
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785])
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
View(y_train)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
y_test <- to_categorical(y_test, num_classes)
knitr::opts_chunk$set(echo = TRUE)
# Load Libraries
library(keras)
library(tidyverse)
library(caret)
# Load data
load("data/sketches_train.rda")
load("data/sketches_test.rda")
# Data Preparation
batch_size <- 128
num_classes <- 6
epochs <- 12
# Input image dimensions
img_rows <- 28
img_cols <- 28
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785])
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785])
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
View(y_train)
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785]) -1
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785]) -1
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
scores <- model %>% evaluate(
x_test, y_test, verbose = 0
)
# Output metrics
cat('Test loss:', scores[[1]], '\n')
cat('Test accuracy:', scores[[2]], '\n')```
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785]) -1
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785])
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
rm(list=ls())
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
# Load Libraries
rm(list=ls())
library(keras)
library(tidyverse)
library(caret)
# Load data
load("data/sketches_train.rda")
load("data/sketches_test.rda")
# Data Preparation
batch_size <- 128
num_classes <- 6
epochs <- 12
# Input image dimensions
img_rows <- 28
img_cols <- 28
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785]) -1
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785])
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
View(y_train)
?fit
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
scores <- model %>% evaluate(
x_test, y_test, verbose = 0
)
View(y_train)
knitr::opts_chunk$set(echo = TRUE)
# Load Libraries
rm(list=ls())
library(keras)
library(tidyverse)
library(caret)
library(tidyr)
library(ggplot2)
# Load data
load("data/sketches_train.rda")
load("data/sketches_test.rda")
# Data Preparation
batch_size <- 128
num_classes <- 6
epochs <- 10
# Input image dimensions
img_rows <- 28
img_cols <- 28
class_names = c('banana',
'boomerang',
'cactus',
'crab',
'flip flops',
'kangaroo')
sketches <- sketches[sample(nrow(sketches)),]
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
train_images <- data.matrix(sketches[,1:784])
train_labels <- data.matrix(sketches[,785])-1
test_images <- data.matrix(sketches_test[, 1:784])
#test_labels <- data.matrix(sketches_test[, 785])-1
# Redefine  dimension of train/test inputs
train_images <- array_reshape(train_images, c(nrow(train_images), img_rows, img_cols, 1))
test_images <- array_reshape(test_images, c(nrow(test_images), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
train_labels[1:20]
# Transform greyscale values into [0,1] range
train_images <- train_images / 255
test_images <- test_images / 255
mea <- mean(train_images[,,,])
sds <- sd(train_images[,,,])
train_images <- (train_images - mea) / sds
test_images <- (test_images - mea) / sds
cat('train_images_shape:', dim(train_images), '\n')
cat(nrow(train_images), 'train samples\n')
cat(nrow(test_images), 'test samples\n')
# Convert class vectors to binary class matrices
train_labels <- to_categorical(train_labels, num_classes)
#test_labels <- to_categorical(test_labels, num_classes)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x=train_images,
y=train_labels,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
scores <- model %>% evaluate(
test_images, test_labels, verbose = 0
)
knitr::opts_chunk$set(echo = TRUE)
# Load Libraries
rm(list=ls())
library(keras)
library(tidyverse)
library(caret)
library(tidyr)
library(ggplot2)
# Load data
load("data/sketches_train.rda")
load("data/sketches_test.rda")
# Data Preparation
batch_size <- 128
num_classes <- 6
epochs <- 10
# Input image dimensions
img_rows <- 28
img_cols <- 28
class_names = c('banana',
'boomerang',
'cactus',
'crab',
'flip flops',
'kangaroo')
sketches <- sketches[sample(nrow(sketches)),]
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
train_images <- data.matrix(sketches[,1:784])
train_labels <- data.matrix(sketches[,785])-1
test_images <- data.matrix(sketches_test[, 1:784])
#test_labels <- data.matrix(sketches_test[, 785])-1
# Redefine  dimension of train/test inputs
train_images <- array_reshape(train_images, c(nrow(train_images), img_rows, img_cols, 1))
test_images <- array_reshape(test_images, c(nrow(test_images), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
train_images <- train_images / 255
test_images <- test_images / 255
mea <- mean(train_images[,,,])
sds <- sd(train_images[,,,])
train_images <- (train_images - mea) / sds
test_images <- (test_images - mea) / sds
cat('train_images_shape:', dim(train_images), '\n')
cat(nrow(train_images), 'train samples\n')
cat(nrow(test_images), 'test samples\n')
# Convert class vectors to binary class matrices
train_labels <- to_categorical(train_labels, num_classes)
#test_labels <- to_categorical(test_labels, num_classes)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_sparse_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x=train_images,
y=train_labels,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x=train_images,
y=train_labels,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
# scores <- model %>% evaluate(
#   test_images, test_labels, verbose = 0
# )
# Output metrics
cat('Test loss:', scores[[1]], '\n')
# Model fitting -----------------------------------------------------------
# callbacks for weights and learning rate
input_img <- layer_input(shape = c(28, 28, 1))
model2 <- application_densenet(include_top = TRUE, input_tensor = input_img, dropout_rate = 0.2)
?application_densenet
# Model fitting -----------------------------------------------------------
# callbacks for weights and learning rate
input_img <- layer_input(shape = c(28, 28, 1))
model2 <- application_densenet(include_top = TRUE, input_tensor = input_img, classes = 6)
keras.applications
?keras.applications
??keras.applications
?densenet_preprocess_input
tensorflow.keras.applications.densenet
evaluate(model, x_test, y_test)
# Libraries ---------------------------------------------------------------
library(keras)
#' In this example we will train a DenseNet-40-12 to classify images from the
#' CIFAR10 small images dataset. This takes ~125s per epoch on a NVIDIA GEFORCE 1080 Ti,
#' so using a GPU is highly recommended.
#'
#' [DenseNet](https://arxiv.org/abs/1608.06993) is a network architecture where each
#' layer is directly connected to every other layer in a feed-forward fashion
#' (within each dense block). For each layer, the feature maps of all preceding
#' layers are treated as separate inputs whereas its own feature maps are passed on as
#' inputs to all subsequent layers. This connectivity pattern yields state-of-the-art
#' accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale
#' ILSVRC 2012 (ImageNet) dataset, DenseNet achieves a similar accuracy as ResNet, but using
#' less than half the amount of parameters and roughly half the number of FLOPs.
#'
#' Final accuracy on test set was 0.9351 versus 0.9300 reported on the
#' [paper](https://arxiv.org/abs/1608.06993).
#'
#' Beside the `keras` package, you will need to install the `densenet` package.
#' Installation instructions are available [here](https://github.com/dfalbel/densenet).
#'
# Libraries ---------------------------------------------------------------
library(keras)
library(densenet)
insta
install.packages("densenet")
install.packages("resnet")
install.packages(c("backports", "C50", "car", "carData", "dbplyr", "dplyr", "ggplot2", "haven", "httpuv", "keras", "modelr", "pkgload", "reticulate", "rmarkdown", "rversions", "sp", "tidyr", "tinytex", "xfun"))
