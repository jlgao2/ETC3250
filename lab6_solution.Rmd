---
title: "Lab 6 Solution"
subtitle: "Monash University, Econ & Bus Stat, ETC3250/5250"
author: "prepared by Professor Di Cook"
date: "Materials for Week 6"
output:
  html_document: default
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 6,
  fig.align = "center",
  cache = FALSE
)
```

# Objective

The objectives for this week are:

- Practice fitting a classification tree model
- Understand the way the tree model is fitted based on impurity
- Learn about the relationship between fitting parameters, bias and variance

# Class discussion 

We will focus on two of the three main groups, north, sardinia, in the olive oil data, and think about how to best separate the two groups using the eight variables. 

```{r out.width="100%", fig.width=8}
library(tidyverse)
library(rpart)
library(knitr)
library(kableExtra)
olive <- read_csv("http://www.ggobi.org/book/data/olive.csv") %>%
  rename(name=X1) %>%
  filter(region != 1) %>% # Drop oils from South
  dplyr::select(-name, -area) %>%
  mutate(region = factor(region))
kable(olive[1:5,]) %>% kable_styling()
```

*Question 1: Here are plots of two pairs of variables. If you got to choose two variables for splitting the two groups, which would you choose, oleic or arachidic, in association with linoleic? Why?*

```{r out.width="100%", fig.width=8}
library(gridExtra)
p1 <- ggplot(olive, aes(x=linoleic, y=oleic, colour=region)) + 
  geom_point() + scale_colour_brewer("", palette="Dark2") +
  theme(legend.position="none", aspect.ratio=1) 
p2 <- ggplot(olive, aes(x=linoleic, y=arachidic, colour=region)) + 
  geom_point() + scale_colour_brewer("", palette="Dark2") +
  theme(legend.position="none", aspect.ratio=1) 
grid.arrange(p1, p2, ncol=2)
```

*Question 2: For the olive oil data set, the classification tree will use just one of the possible eight variables for its model. It splits on linoleic acid as shown. Why do you think the tree fitting algorithm chose this variable? There is no gap between the groups. What problem might this create with future data? Why?*

```{r out.width="70%"}
rpart(region~., data=olive)
ggplot(olive, aes(x=linoleic, y=region, colour=region)) + 
  geom_point(alpha=0.5) + scale_colour_brewer("", palette="Dark2") +
  theme(legend.position="none") +
  geom_vline(xintercept=1053.5)
```

*Question 3: Suppose you work with linoleic and arachidic. Would quadratic discriminant analysis produce a better separation than the tree? Argue your viewpoint.*

*Question 4: Find a linear combination of linoleic and arachidic, and create a new variable to pass to the tree. Re-fit the tree with this variable instead of the original two. What does the model look like now? Is this better than the original tree?*

*Question 5: In general, why is it often important to create new variables (feature engineering) when building models?*


# Practice

1. This question is about entropy as an impurity metric  fora classification  tree.
    a. Write down the formula for entropy as an impurity measure for two groups. 
    $$- \hat{p}_{1} log_2(\hat{p}_{1}) -  \hat{p}_{2} log_2(\hat{p}_{2})$$
    b. Establish that the the worst case split has 50% one group and 50% the other group, in whatever way you  would like (algebraicly or graphically).
```{r}
p <- seq(0.01, 0.99, 0.01)
y <- -p*log(p)-(1-p)*log(1-p)
df <- tibble(p, y)
ggplot(df, aes(x=p, y=y)) + geom_line() + ylab("entropy")
```
**The highest value occurs when $p=0.5$, which is the worst possible value the impurity can take.**
    c. Extend the entropy formula so that it can be used to describe the  impurity for a possible  split of the data into two subsets. That is, it needs  to be the sum of the impurity for both left and right subsets of data.
    **Let $L$ indicate the subset of observations to the left of the split, and $R$ indicate those to the right.**
$$p_L(- \hat{p}_{L1} log_2(\hat{p}_{L1}) -  \hat{p}_{L2} log_2(\hat{p}_{L2}))  + p_R(-\hat{p}_{R1} log_2(\hat{p}_{R1}) - \hat{p}_{R2} log_2(\hat{p}_{R2}))$$ 
3. For this sample of data, 
```{r}
df <- tibble(x=c(1,3,4,5,7), y=c("A", "B", "A", "B", "B"))
kable(df) %>% kable_styling()
```
    a. compute the entropy impurity metric for all possible splits.
```{r}
splits <- tibble(split=c(2, 3.5, 4.5, 6), 
                 impurity = c(4/5*(-1/4*log(1/4)-3/4*log(3/4)), 
                              2/5*(-2*1/2*log(1/2))+3/5*(-1/3*log(1/3)-2/3*log(2/3)),
                              3/5*(-2/3*log(2/3)-1/3*log(1/3)),
                              4/5*(-2*1/2*log(1/2))) )
splits
```
    b. Write  down the classification rule for the tree that would  be formed for the best split.
**If $x>4.5$ classify new observation to group B.**    
4. For the following data set, compute. Write out the decision  tree, and also sketch the boundary between classes.
    a. olive oils,  for three regions
```{r}
olive <- read_csv("http://www.ggobi.org/book/data/olive.csv") %>%
  rename(name=X1) %>%
  dplyr::select(-name, -area) %>%
  mutate(region = factor(region))
olive_rp <- rpart(region~., data=olive)
olive_rp
ggplot(olive, aes(x=eicosenoic, y=linoleic, colour=region)) +
  geom_point() + 
  scale_color_brewer("", palette="Dark2") +
  geom_vline(xintercept=6.5) +
  annotate("line", x=c(0, 6.5), y=c(1053.5, 1053.5))
```
    b. chocolates, for type
```{r}
choc <-  read_csv("data/chocolates.csv") %>%
  select(Type:Protein_g)
choc_rp <- rpart(Type~., data=choc)
choc_rp
ggplot(choc, aes(x=Fiber_g, y=CalFat, colour=Type)) +
  geom_point() + 
  scale_color_brewer("", palette="Dark2") +
  geom_vline(xintercept=4.83) +
  annotate("line", x=c(0, 4.83), y=c(337.7, 337.7))
```
    c. flea
```{r}
library(tourr)
flea_rp <- rpart(species~., data=flea)
flea_rp
ggplot(flea, aes(x=aede3, y=tars1, colour=species)) +
  geom_point() + 
  scale_color_brewer("", palette="Dark2") +
  geom_vline(xintercept=93.5) +
  annotate("line", x=c(93.5, 123), y=c(159, 159))

```

5. For the crabs data, make a new variable combining species and gender into one class variable.
    a. Use the  grand and guided tour with the LDA index to  examine  the data. Describe the shape. Between LDA and a classification  tree which d  youo expect to perform better on this  data?
    ```{r}
crabs <- read_csv("http://www.ggobi.org/book/data/australian-crabs.csv") %>%
  mutate(class = interaction(species, sex)) %>%
  dplyr::select(-index, -species,-sex)
```
**The variables are highly correlated, and the difference between groups uses a combination of variables. Trees will have a difficult time  with this data. LDA should perform better.**

    b. Fit the default decision tree, using species and gender. Explain why it  is so complicated.
```{r results='hide'}
crabs_rp <- rpart(class~., crabs)
crabs_rp
```
**It is really hard for a tree to separate this data.  The tree can only use a single variable at each split. It needs to iterate many times between single variables because the ideal boundary uses a combination of variables.**

    c. Break the  data into 50% training data, and 50% test data, ensuring that  sampling is done within the class variable. Change the options for the tree to fit it the training data with increasingly well. Compute the training and test error for each of the options and plot these. What best options would be suggested  based on minimising test error?
```{r}
library(caret)
set.seed(20200429)
tr_indx <- createDataPartition(crabs$class, p=0.5)$Resample1
crabs_tr <- crabs[tr_indx,]
crabs_ts <- crabs[-tr_indx,]
rp_ctl <- tibble(indx=10:1, minsplit=c(3, 6, 9, 12, 15, 18, 24, 27, 30, 45), cp=rep(0.005, 10))
tr_err <- NULL; ts_err <- NULL
for (i  in 1:nrow(rp_ctl)) {
   crabs_tr_rp <- rpart(class~., crabs_tr,
                        control = rpart.control(minsplit=rp_ctl$minsplit[i],
                                                cp=rp_ctl$cp[i]))
   tr_err <- c(tr_err, 
               1-confusionMatrix(predict(crabs_tr_rp, crabs_tr, type = "class"),
                   crabs_tr$class)$overall[1])
   ts_err <- c(ts_err, 
               1-confusionMatrix(predict(crabs_tr_rp, crabs_ts, type = "class"),
                   crabs_ts$class)$overall[1])
}
crabs_err <- tibble(tr_err, ts_err, indx = rp_ctl$indx,
                    minsplit=rp_ctl$minsplit, 
                    cp=rp_ctl$cp) %>%
  pivot_longer(cols=c(tr_err, ts_err), names_to = "type", values_to = "error")
ggplot(crabs_err, aes(x=indx, y=error, colour=type)) + xlab("flexibility") +
  geom_line() + scale_colour_brewer("", palette="Dark2")
```
**Around a minplit of 12 provides a reasonably low test error, without the training errr going to 0. It should be noted that the results change substantially if a different seed is used. Regardless of the seed the test error tends to keep decreasing. This is likely because the tree is struggling against the big problem of separation between classes being in a combination f variables.**


