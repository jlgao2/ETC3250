---
title: "ETC3250 ASSIGNMENT 4"
author: "Jia Lin Gao (25982990), Yang Wang (28463293), Alicia Lam (29676088), Tarushi Hondhe-Munige-Leader (28768361)"
date: "17/05/2020"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE)
suppressWarnings(RNGversion("3.5.3"))
library(tidyverse)
library(ISLR)
library(xgboost)
library(caret)
library(mlr)
library(gbm)
library(knitr)
library(kableExtra)
library(e1071)
library(kernlab)
library(randomForest)
library(plotly)
library(GGally)
```

## Exercises

### 1. 

Here we explore the maximal margin classifier on a toy data set.

```{r eval=TRUE}
df <- tibble(id=1:15, x1=c(2, 4, 1, 4, 3, 4, 2, 5, 3, 1, 1, 2, 0, 1, 0), 
             x2=c(4, 2, 5, 4, 3, 3, 5, 3, 1, 3, 1, 1, 2, 2, 3), 
             class=c(rep(-1, 8), rep(1, 7)))
kable(df) %>% kable_styling()
```

a. We are given $n = 15$ observations in $p = 2$ dimensions. For each observation, there is an associated class label. Sketch the observations.

```{r}
p1 <- ggplot(data=df, mapping=aes(x=x1, y=x2, color=as_factor(class)))+
  geom_point() +
  geom_label(aes(label = id), nudge_x = 0.2) +
  labs(title = "Toy Dataset\n", x = "x1", y = "x2", color = "Class\n") 
p1
```

b. Sketch the optimal separating hyperplane, and provide the equation for this hyperplane in the form of textbook equation 9.1. 

```{r}
p2 <- p1 + geom_abline(aes(intercept = 4.5,  slope = -1))
p2 
```

$$\beta_0 +\beta_1 X_1 + \beta_2 X_2 = 0$$

$$ 4.5 - X_1 - X_2 = 0  $$

c. Write the classification rule for the maximal margin classifier.

$$Classify\ to -1\ if X_1 - X_2 + 4.5 <0, otherwise, classify\ to\ 1$$

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 > 0\ if Y_i = 1$$

$$-4.5995 - 0.7991 X_1 - 1.1999 X_2 > 0\ if\ Y_i = 1$$
$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 < 0\ if\ Y_i = 1$$

$$-4.5995 - 0.7991 X_1 - 1.1999 X_2 < 0\ if\ Y_i = 1$$

d. On your sketch, indicate the margin for the maximal margin hyperplane.

```{r}
p3 <- p2 + geom_abline(aes(intercept = 4,  slope = -1),  linetype="dashed")+
  geom_abline(aes(intercept = 5, slope = -1), linetype="dashed")
  
p3 
```

- The margin is the distance between the classifier line to the dashed lines 

e. Indicate the support vectors for the maximal margin classifier.

```{r}
p4 <- p3 + 
  geom_segment(mapping=aes(x = 1, xend = 1, y=4, yend=3.5), color="black", arrow=arrow()) +
  geom_segment(mapping=aes(x = 1, xend = 1, y=3, yend=3.5), color="black", arrow=arrow()) +
  geom_segment(mapping=aes(x = 3, xend = 3, y=1, yend=1.5), color="black", arrow=arrow()) +
  geom_segment(mapping=aes(x = 3, xend = 3, y=2, yend=1.5), color="black", arrow=arrow()) 
p4
```

- The support vectors are the points 2, 3, 9, and 10 for this problem.

f. Argue that a slight movement of observation 4 would not affect the maximal margin hyperplane.

- The maximal margin hyperplane is only affected by the support vectors. Observation 4 is not a support vector, and therefore the maximal marginal hyperplane is not affected unless observation 4 moves to within the margin of the classier.

g. Sketch a separating hyperplane that is *not* the optimal separating hyperplane, and provide the equation for this hyperplane.

- A hyperplane that is not optimal would be any value less than 4 and greater than 4.5.

```{r}
p5 <- p1 + geom_abline(aes(intercept = 4,  slope = -1),  linetype="dashed")+
  geom_abline(aes(intercept = 5, slope = -1), linetype="dashed") +
  geom_abline(aes(intercept = 4.25, slope = -0.9))
  
p5
```

- The equation of this separating hyperplane is:

$$ 4.25 - X_1 - 0.9X_2 $$

h. How would the separating hyperplane change if an 8th observation (16, 2, 2.5, 1) as added to the data? 

```{r}
df_mod <- add_row(df, id=16, x1=2, x2=2.5, class=1)
p6 <- ggplot(data=df_mod, mapping=aes(x=x1, y=x2, color=as_factor(class)))+
  geom_point() +
  geom_label(aes(label = id), nudge_x = 0.2) +
  labs(title = "Toy Dataset + New Observation\n", x = "x1", y = "x2", color = "Class\n") 
  
p6
```

```{r}
a = sqrt(1/403)

p7 = p6 + geom_abline(aes(intercept = 19/4,  slope = -1))
p7
```

Using gaussian elimination on the three nearest points (2, 3, 16) in matrix form
$$ \begin{bmatrix} 1 & 2 & 2.5 & M \\ -1 & -3 & -2 & M \\ -1 & -1 & -4& M \end{bmatrix} $$

Reduces to 

$$ \begin{bmatrix} 1 & 2 & 2.5 & M \\ 0 & -1 & 0.5 & M \\ 0 & 0 & -1& 4M \end{bmatrix} $$

Which yields

$$\beta_0=19M, \beta_1 = -4M, \beta_2 = -4M$$

Subject to 

$$ \sum{\beta_j^2 = 1} $$

$$ M = \sqrt{\frac{1}{403}}$$

And the classification equation is

$$ y_i(19M - 4Mx_i - 4Mx_i2) > 0   $$

- The intercept of the hyperplane has to be changed from the previous optimal hyperplane, as there is a new interception at the new observation.

i. Using the `svm` function of the `e1071` package fit the linear svm model to this data. Compare the result with your hand calculation.

```{r}
mutate(df_mod, class=as_factor(class))
svm_1 <- svm(class ~ x1 + x2, data = df_mod, kernel="linear", scale = FALSE, type = "C-classification")
print(svm_1)
summary(svm_1)
cf <- coef(svm_1)
p8 <- p7 + geom_abline(mapping=aes(intercept=-cf[1]/cf[3], slope=-cf[2]/cf[3]), col = "red") +
   labs(title = "Red line is SVM Function Classifier\n")
p8
```


### 2. 

Use the `Caravan` data from the `ISLR` package. Read the data description.

a. Compute the proportion of caravans purchased relative to not purchased. Is this a balanced class data set? What problem might be encountered in assessing the accuracy of the model as a consequence?

```{r}
data(Caravan)
Caravan %>% count(Purchase)
```

- With 5474 not purchased and 348 purchased it is clearly seen that this is not a balanced dataset as the event rate is at around 5.98%, with the $No$ class is far larger than the $Yes$ class.
- This may cause problems in training, because the majority class can dominate the minority class. Classification results using a model thus trained may be accurate only due to the underlying distribution being learnt, rather than any differentiating features. Oversampling has been examined to see whether that would improve results in the Appendix. 

b. Convert the response variable from a factor to an integer variable, where 1 indicates that the person purchased a caravan.

```{r}
mycaravan <- Caravan %>% 
  mutate(Purchase = as.integer(ifelse(Caravan$Purchase == "Yes", 1, 0)))
```

c. Break the data into 2/3 training and test set, ensuring that the same ratio of the response variable is achieved in both sets. Check that your sampling has produced this.

```{r}
set.seed(1219947272)
tr_indx <- createDataPartition(mycaravan$Purchase, p=2/3)$Resample1
c_tr <- mycaravan[tr_indx,]
c_ts <- mycaravan[-tr_indx,]
check_tr <- c_tr %>% count(sum(Purchase == 1))
check_ts <- c_ts %>% count(sum(Purchase ==1))
check_tr[,1]/check_tr[,2]
check_ts[,1]/check_ts[,2]
```

d. The solution code on the unofficial solution web site:

```
library(ISLR)
train = 1:1000
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train = Caravan[train, ]
Caravan.test = Caravan[-train, ]
```
would use just the first 1000 cases for the training set. What is wrong about doing this?

- By using only the first 1000 cases, there is no randomisation in the sample obtained, and therefore an accurate training set that represents the overall dataset is not obtained.
- However, there are 59 purchases in the first 1000 entries, which shows that this subset is similar in the response variable to the dataset. 
- A smaller training dataset may result in the model being prone to overfitting.

```{r}
mycaravan %>% 
  head(1000) %>% 
  count(sum(Purchase == 1))
```


### 3. 

Here we will fit a boosted tree model, using the `gbm` package.  

a. Use 1000 trees, and a shrinkage value of 0.01. 

```{r}
c_boost = gbm(Purchase ~., data = c_tr, n.trees = 1000, shrinkage = 0.01, 
    distribution = "bernoulli")
head(summary(c_boost, plotit=FALSE), 6)
```

b. Make a plot of the oob improvement against iteration number. What does this suggest about the number of iterations needed? Why do you think the oob improvement value varies so much, and can also be negative?

```{r}
c_boost_diag <- tibble(iter=1:1000, tr_err=c_boost$train.error, oob_improve=c_boost$oobag.improve)
ggplot(c_boost_diag, aes(x=iter, y=oob_improve)) + geom_point() + geom_smooth() +
  xlab("Number of Iterations") + ylab("OOB Improvement")
```

- At approximately 500 iterations, the out of bag (OOB) improvement plateaus. This showcases that only 500 iterations are needed for the maximum accuracy of the prediction model to be reached.
- OOB improvement values can be negative. This would occur if the model was overfitted for the training set, there would be an increase in misclassifications in the test set, therefore resulting in negative OOB-improvement values. 

c. Compute the error for the test set, and for each class. Consider a proportion 0.2 or greater to indicate that the customer will purchase a caravan.

```{r}
boost.prob = predict(c_boost, c_ts, n.trees = 1000, type = "response")
boost.pred = ifelse(boost.prob > 0.2, 1, 0)
addmargins(table(c_ts$Purchase, boost.pred))
```

- The test error with the $gbm$ model for the purchase class is 84.5%, the test error for the no purchase class is 3.7%, and the total test error is 8.6%

d. What are the 6 most important variables? Make a plot of each to examine the relationship between these variables and the response. Explain what you learn from these plots.

> The boosted tree model suggested that the following six variables are most important, with the attached relative importance figures. 

```
Variable  Description                 Relative Importance
PPLEZIER	Contribution boat policies	24.142909		
PPERSAUT	Contribution car policies	  23.061736		
PBRAND	  Contribution fire policies	6.043500		
ALEVEN	  Number of life insurances	  4.755354		
APERSAUT	Number of car policies    	3.695288		
MKOOPKLA	Purchasing power class	    3.043218	
```

```{r}
bstVars <- c('PPLEZIER', 'PPERSAUT', 'PBRAND', 'ALEVEN', 'APERSAUT', 'MKOOPKLA')
i = 0
pltList <- list()

for (var in bstVars){
  i <- i+1
  p <- ggplot(data = Caravan, mapping = aes_string(x=var, color='Purchase')) + 
    geom_histogram(aes(y=..density..), alpha=0.2, position="identity") +
    geom_density()
  pltList[[i]] <- p
  }

grid.arrange(pltList[[1]], pltList[[2]],pltList[[3]],pltList[[4]],pltList[[5]],pltList[[6]], nrow = 3)
```

- People who purchase caravan policies are more likely to have contributions for car, fire and boat policies. They have more life and car policies and they tend to be slightly more well off than the average insurance customer *in this sample*. 


### 4. 

Here we will fit a random forest model, using the `randomForest` package.  

a. Use 1000 trees, using a numeric response so that predictions will be a number between 0-1, and set `importance=TRUE`. (Ignore the warning about not having enough distinct values to use regression.)

```{r}
c_rf <- randomForest(Purchase ~., data=c_tr, ntree=1000, importance=TRUE)
```

b. Compute the error for the test set, and for each class. Consider a proportion 0.2 or greater to indicate that the customer will purchase a caravan.

```{r}
rf.prob <- predict(c_rf, newdata=c_ts)
rf.pred <- ifelse(rf.prob > 0.2, 1, 0)
addmargins(table(c_ts$Purchase, rf.pred))
```
The test error with the $rf$ model for the purchase class is 73.3%, the test error for the no purchase class is 10.3% for a total test error of 14.1%

c. What are the 6 most important variables? Make a plot of any that are different from those chosen by `gbm`. How does the set of variables compare with those chosen by `gbm`.

```{r}
as_tibble(c_rf$importance) %>% 
  bind_cols(var=rownames(c_rf$importance)) %>%
  arrange(desc(IncNodePurity)) %>% print(n=6)
```

```
Variable   Desciption                 %IncMSE       Inc Node Purity
MOSTYPE    Customer Subtype           4.020756e-03	6.903134e+00	 		
PBRAND     Contribution fire policies 5.697232e-04	6.231104e+00			
PPERSAUT   Contribution car policies	1.313657e-03	5.604083e+00			
MOPLMIDD   Medium level education     1.743855e-03	4.524628e+00			
MKOOPKLA   Purchasing power class	    1.974170e-03	4.329415e+00			
MGODGE     No religion                9.999202e-04	4.190742e+00	
```

```{r}
rfVars <- c('MOSTYPE', 'MOPLMIDD', 'MGODGE')
i = 0
for (var in rfVars){
  i <- i+1
  p <- ggplot(data = Caravan, mapping = aes_string(x=var, color='Purchase')) + 
    geom_density() + 
    geom_histogram(aes(y=..density..), alpha=0.2, position="identity")
  pltList[[i]] <- p
  }

grid.arrange(pltList[[1]], pltList[[2]],pltList[[3]], nrow = 2)
```

- The new variables chose by the $randomForest$ package are MOSTYPE - Customer Subtype, MOPLMIDD - Medium level education, and MGODGE - No religion. We see that the people who buy Caravans are more likely to be MOSTYPE 0-20, which correlate to higher income subgroups such as "2 - Very Important Provincials" or "13 - Young all american family". It also appears that caravan purchasing folks are also more likely to have middle level of education and tend to be more religious. 

### 5. 

Here we will fit a gradient boosted model, using the `xgboost` package.

a.  Read the description of the XGBoost technique at https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/, or other sources. Explain how this algorithm might differ from earlier boosted tree algorithms.

b. Tune the model fit to determine how many iterations to make. Then fit the model, using the parameter set provided.

```{r}
c_tr_xg <- xgb.DMatrix(data = as.matrix(c_tr[,1:85]), label = c_tr[,86])
c_ts_xg <- xgb.DMatrix(data = as.matrix(c_ts[,1:85]), label = c_ts[,86])

params <- list(booster = "gbtree", 
               objective = "binary:logistic", 
               eta=0.3, 
               gamma=0, 
               max_depth=6, 
               min_child_weight=1, 
               subsample=1, 
               colsample_bytree=1)

xgbcv <- xgb.cv(params = params, 
                data = c_tr_xg, 
                nrounds = 300, 
                nfold = 5, 
                showsd = T, 
                stratified = T, 
                maximize = F, 
                early_stopping_rounds = 20)
```

```{r}
xgbcv

min(xgbcv$test_error_mean)

c_xgb <- xgb.train(params = params, data = c_tr_xg, nrounds = 5, watchlist = list(val=c_ts_xg,train=c_tr_xg), maximize = F , eval_metric = "error")
```

b. Compute the error for the test set, and for each class. Consider a proportion 0.2 or greater to indicate that the customer will purchase a caravan.

```{r}
xgbprob <- predict(c_xgb, newdata=c_ts_xg)
xgbpred <- ifelse(xgbprob > 0.2, 1, 0)
addmargins(table(c_ts$Purchase, xgbpred))
```
The test error with the $xgb$ model is 56.90% in the purchase class, 13.82% in the non-purchase class, and 16.39% overall.    

c. Compute the variable importance. What are the 6 most important variables? Make a plot of any that are different from those chosen by `gbm` or `randomForest`. How does the set of variables compare with the other two methods.

```{r}
c_xgb_importance <- xgb.importance(feature_names = colnames(c_tr[,1:85]), model = c_xgb)
head(c_xgb_importance, 6)
```

```
Feature   Description          Gain       Cover       Frequency
MOPLHOOG	High level education 0.03290865	0.01464995	0.04210526		
```
```{r}
rfVars <- c('MOPLHOOG')
i = 0
pltList <- list()

for (var in rfVars){
  i <- i+1
  p <- ggplot(data = Caravan, mapping = aes_string(x=var, color='Purchase')) + 
    geom_density() + 
    geom_histogram(aes(y=..density..), alpha=0.2, position="identity")
  pltList[[i]] <- p
  }

grid.arrange(pltList[[1]], nrow = 1)
```

- This set of variables reveals an increased likelihood of caravan insurers having a higher level of high level education. The most important variables for each of the three methods are tabulated here:

gbm 	    rf	      xgb
PPLEZIER	MOSTYPE  	PPERSAUT
PPERSAUT	PBRAND	  PPLEZIER
PBRAND	  PPERSAUT  PBRAND
ALEVEN	  MOPLMIDD	MKOOPKLA
APERSAUT	MKOOPKLA 	MOSTYPE
MKOOPKLA	MGODGE	  MOPLHOOG


### 6. Compare and summarise the results of the three model fits. 

The results show that the $gbm$ model overall has the highest performance, followed by the $rf$ model while the $xgb$ model shows the lowest overall performance. However this is deceptive as the data is heavily imbalanced. 

The $gbm$ model shows high performance on this dataset as it is able to predict the majority class without capturing any information on the minority class, whereas the $xgb$ model is clearly able to capture more information about the minority class at the expense of overall predictive ability, and the $rf$ model is somewhere in the middle. 

The implication is that the $xgb$ model may be more useful despite higher overall loss in a practical application to predict customer who are likely interested in caravan insurance.


### 7. 

Now scramble the response variable (Purchase) using permutation. The resulting data has no true relationship between the response and predictors. Re-do Q5 with this data set. Write a paragraph explaining what you learn about the true data from analysing this permuted data.

##### Break the data into 2/3 training and test set, ensuring that the same ratio of the response variable is achieved in both sets. Check that your sampling has produced this. Tune the model fit to determine how many iterations to make. Then fit the model, using the parameter set provided.

```{r}
c_tr_xg_scmbl <- xgb.DMatrix(data = as.matrix(c_tr[,1:85]), 
                             label = sample(c_tr[,86]))

c_ts_xg_scmbl <- xgb.DMatrix(data = as.matrix(c_ts[,1:85]), 
                             label = sample(c_ts[,86]))

xgbcv_scmbl <- xgb.cv(params = params, 
                data = c_tr_xg_scmbl, 
                nrounds = 300, 
                nfold = 5, 
                showsd = T, 
                stratified = T, 
                maximize = F, 
                early_stopping_rounds = 20)
```

```{r}
c_xgb_scmbl <- xgb.train(params = params, data = c_tr_xg, nrounds = 10, watchlist = list(val=c_ts_xg,train=c_tr_xg), maximize = F , eval_metric = "error")
```

##### Compute the error for the test set, and for each class. Consider a proportion 0.2 or greater to indicate that the customer will purchase a caravan.

```{r}
xgbprob <- predict(c_xgb_scmbl, newdata=c_ts_xg_scmbl)
xgbpred <- ifelse(xgbprob > 0.2, 1, 0)
addmargins(table(c_ts$Purchase, xgbpred))
```

- The overall error rate is actually lower in this sample at 11.65%. However the error rate on the purchase class however is higher than the unscrambled data at 72.41%. This increase in error with permutation shows that the original $xgb$ model does indeed capture useful relationship from the data for predicting the class purchase. 


### References
https://machinelearningmastery.com/what-is-imbalanced-classification/
