---
title: "ETC3250/5250 Assignment 2"
date: "DUE: Friday, Apr 24 5pm "
author: "Jia Lin Gao - leader (25982990), Yang Wang (28463293), Alicia Lam (29676088), Tarushi Hondhe-Munige (28768361)"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE)
```


## Question 1

This question is about the normal distribution, and how it relates to the classification rule provided by quadratic discriminant analysis.

a. Write down the density function for a univariate normal distribution ($p=1$), with mean $\mu_k$ and variance $\sigma_k$. 

  $$f(x)=\frac{1}{\sigma_k\sqrt2\pi}e^{\frac{-1}{2}(\frac{x-\mu_k}{\sigma_k})^2}$$

b. Show that the quadratic discriminant rule for two groups ($K=2$), $\pi_1=\pi_2$ is equal to:

*Assign a new observation $x_0$ to group 1 if*

$$-\frac12 \left( \frac{1}{\sigma_1^2}- \frac{1}{\sigma_2^2} \right) {x_0^2} + \left(\frac{\mu_1}{\sigma_1^2}-\frac{\mu_2}{\sigma_2^2}\right)x_0 -\frac12 \left(\frac{\mu_1^2}{\sigma_1^2}-  \frac{\mu_2^2}{\sigma_2^2} \right)  -\log{\sigma_1}+\log{\sigma_2}>0$$


$$p_1(x_0) > p_2(x_0)$$


$$\rightarrow \frac{\pi_1\frac{1}{\sqrt{2 \pi} \sigma_1} \text{exp}\left( - \frac{1}{2 \sigma_1^2} (x_0 - \mu_1)^2 \right) }{ \sum_{l = 1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma_1} \text{exp} \left( - \frac{1}{2 \sigma_1^2} (x - \mu_l)^2 \right) } > \frac{\pi_2\frac{1}{\sqrt{2 \pi} \sigma_2} \text{exp}\left( - \frac{1}{2 \sigma_2^2} (x_0 - \mu_2)^2 \right) }{ \sum_{l = 1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma_2} \text{exp} \left( - \frac{1}{2 \sigma_2^2} (x - \mu_l)^2 \right) } ~~~\textit{cancel common denom}$$   


$$\Rightarrow \pi_1\frac{1}{\sqrt{2 \pi} \sigma_1} \text{exp}\left( - \frac{1}{2 \sigma_1^2} (x_0 - \mu_1)^2 \right) > \pi_2\frac{1}{\sqrt{2 \pi} \sigma_2} \text{exp}\left( - \frac{1}{2 \sigma_2^2} (x_0 - \mu_2)^2 \right) ~~~\textit{cancel } \pi_1 = \pi_2 \textit{ and remove const, take natural log }  $$


$$\Rightarrow -\log(\sigma_1) - \frac{1}{2 \sigma^2} (x_0 - \mu_1)^2  > -\log(\sigma_2)  - \frac{1}{2 \sigma^2} (x_0 - \mu_2)^2 ~~~\textit{shift sides, expand}$$


$$\Rightarrow \frac{1}{2\sigma_2^2}(x_0^2-2x_0\mu_2+\mu_2^2) -\frac{1}{2\sigma_1^2}(x_0^2 - 2x_0\mu_1 + \mu_1^2)-\log(\sigma_1)+\log(\sigma_2) > 0 ~~~\textit{put under common denominator & collect like terms}$$


$$\Rightarrow-\frac{1}{2}\frac{(\sigma_2^2-\sigma_1^2)x_0^2+2(\mu_1\sigma_2^2-\mu_2\sigma_1^2)x_0+(\mu_1^2\sigma_2^2-\mu_2^2\sigma_1^2)}{\sigma_1^2\sigma_2^2}- \log(\sigma_1)+\log(\sigma_2) > 0~~~\textit{simplify fractions}$$


$$-\frac12 \left( \frac{1}{\sigma_1^2}- \frac{1}{\sigma_2^2} \right) {x_0^2} + \left(\frac{\mu_1}{\sigma_1^2}-\frac{\mu_2}{\sigma_2^2}\right)x_0 -\frac12 \left(\frac{\mu_1^2}{\sigma_1^2}-  \frac{\mu_2^2}{\sigma_2^2} \right)  -\log{\sigma_1}+\log{\sigma_2}>0 ~~~\textit{Q.E.D.}$$


c. Suppose $\mu_1=4, \mu_2=-5, \sigma_1=0.5, \sigma_2=5$ simulate a set of 50 observations from each population. Make a plot of the population model, and add these samples as a rug plot on the horizontal axis. (See the lecture notes for a similar plot and code for linear discriminant analysis.)
    
```{r fig.width=6, fig.height=4, out.width="80%"}
library(tidyverse)
set.seed("24042020")
n <- 100; n1 <- 50
m1 <- 4
m2 <- -5
s1 <- 0.5
s2 <- 5
x <- c(rnorm(n1, m1, s1), rnorm(n-n1, m2, s2))
y <- c(rep(1, n1), rep(2, n-n1))
df <- tibble(x, y)

x <- seq(-20, 20, 0.2)
dx <- c(dnorm(x, m1, s1), dnorm(x, m2, s2))
y <- factor(c(rep(1, length(x)), rep(2, length(x))))
df_pop <- tibble(x=c(x,x), dx, y)
p <- ggplot() + 
  geom_rug(data=df, aes(x=x, y=0, colour=factor(y)), alpha=0.7) +
  geom_line(data=df_pop, aes(x=x, y=dx, colour=y)) + 
  scale_color_brewer("", palette="Dark2") +
  xlab("x")  + ylab("density")
p
```
   
d. Write down the rule using these parameter values, and sketch the boundary corresponding to the rule on the previous plot.
    
  Substitute the values for $\sigma_k$ and $\mu_k$ into the below equation 
    
$$-\frac12 \left( \frac{1}{\sigma_1^2}- \frac{1}{\sigma_2^2} \right) {x_0^2} + \left(\frac{\mu_1}{\sigma_1^2}-\frac{\mu_2}{\sigma_2^2}\right)x_0 -\frac12 \left(\frac{\mu_1^2}{\sigma_1^2}-  \frac{\mu_2^2}{\sigma_2^2} \right)  -\log{\sigma_1}+\log{\sigma_2}>0 $$
  
To determine quadratic coefficients, then plug values into quadratic equation to solve for boundary. 

```{r}
a = -0.5*((1/s1^2)-(1/s2^2))
b = m1/s1^2 - m2/s2^2
c = -0.5*(m1^2/s1^2-m2^2/s2^2) - log(s1) + log(s2)
x_pos = (-b + sqrt(b^2 - 4*a*c))/(2*a)
x_neg = (-b - sqrt(b^2 - 4*a*c))/(2*a)
p + geom_vline(xintercept=x_pos, linetype=2)
sprintf("QDA boundary is at %f", x_pos)
```
   
e. If instead you had made a mistake and assumed that the two variances were equal, this would have produced a linear discrimant rule. Mark this boundary on the previous plot. Explain why and how this differs from result of the QDA rule.
      
  Using the linear discriminant rule instead of the quadratic discriminant rule in this example would result in worse classification performance as the linear discriminant rule relies on the assumption that the variance/covariance matrix is equal. If the assumption is false then it does not yield the best estimate. We can see in the example that the QDA demarcates a better boundary as it accounts for different variances between categories in this univariate case. LDA overassigns to class 1 due to the lower variance of class 1.  
    
```{r}
p + geom_vline(xintercept=x_pos, linetype=2) +
  geom_vline(xintercept=(m1+m2)/2, linetype=3, colour="red")
```

## Question 2
In this question you are going to practice conducting bootstrap to obtain confidence intervals for a reasonably complicated yet simple analysis. 

*A significant gender gap in maths performance in favour of male students has returned, despite closing in 2015* [Natassia Chrysanthos, Sydney Morning Herald](https://www.smh.com.au/national/nsw/urgent-need-to-address-maths-performance-as-nsw-slumps-in-international-test-20191203-p53ge2.html)

Last December, the 2018 [OECD PISA results](http://www.oecd.org/pisa/data/) were released. These are standardised test scores in math, reading and science, of 15 year olds across the globe. It led to a flurry of articles in the news about slipping standards of Australian students. If you also browsed the news of other countries (including New Zealand, Indonesia, Finland), you would find that many had similarly woeful stories. The above headline focuses on the math gap. To explore this, we will compute bootstrap confidence intervals for the difference between weighted averages for boys and girls in each country. The data is from the 2015 results.[4] 


```{r gender_means}
library(tidyverse)
library(ISOcodes)
data("ISO_3166_1")
#The code block below will compute the difference between weighted averages for boys and girls in each country. A weighted average is often used with survey data, to reflect how the sampling was done relative to the population characteristics. The weighted average will typically better reflect the population mean. 

# Load data
load("data/pisa_scores.rda")

# The country information will be used to join the data with map data 
# and the ISOcodes package provides information about codes and country
scores <- scores %>% 
  mutate(CNT=recode(CNT, "QES"="ESP", "QCH"="CHN", "QAR"="ARG", "TAP"="TWN")) %>%
  filter(CNT != "QUC") %>%
  filter(CNT != "QUD") %>%
  filter(CNT != "QUE") %>%
  mutate(gender=factor(gender, levels=c(1,2), labels=c("female","male")))
score_gap <- scores %>% 
  group_by(CNT, gender) %>%
  summarise(math=weighted.mean(math, w=w, na.rm=T),
            reading=weighted.mean(reading, w=w, na.rm=T),
            science=weighted.mean(science, w=w, na.rm=T)) %>%
  pivot_longer(cols=c(math, reading, science), names_to="test", values_to="score") %>%
  pivot_wider(names_from=gender, values_from=score) %>%
  mutate(gap = male - female) %>%
  pivot_wider(id_cols=CNT, names_from=test, values_from=gap)
```


```{r}
#This block of code will compute 90% bootstrap confidence intervals for the weighted mean difference. 

library(boot)
#library(plyr)
# Compute confidence intervals
cifn <- function(d, i) {
  x <- d[i,]
  ci <- weighted.mean(x$math[x$gender=="male"], w=x$w[x$gender=="male"], na.rm=T)-
                     weighted.mean(x$math[x$gender=="female"], w=x$w[x$gender=="female"], na.rm=T)
  ci
}
bootfn <- function(d) {
  r <- boot(d, statistic=cifn, R=100)
  l <- sort(r$t)[5]
  u <- sort(r$t)[95]
  ci <- c(l, u)
  return(ci)
}
#student2012.sub.summary.gap.boot <- ddply(student2012.sub, .(CNT), bootfn)
score_gap_boot <- scores %>% 
  split(.$CNT) %>% purrr::map(bootfn) %>% as_tibble() %>%
  pivot_longer(cols=everything(), names_to="CNT", values_to="bounds" ) %>%
  arrange(CNT) %>%
  mutate(bound=rep(c("ml","mu"), length(unique(scores$CNT)))) %>%
  pivot_wider(names_from = bound, values_from = bounds)
score_gap <- score_gap %>%
  left_join(score_gap_boot, by="CNT")
```


```{r gap_dots, fig.height=8, fig.width=6, out.width="60%"}
#This block of code will add country names, and make dotplots with confidence intervals for the math gap for each country. 

#wrapped this bit in if statement 
if (!'name' %in% colnames(score_gap)){
  score_gap <- score_gap %>%
  left_join(ISO_3166_1[,c("Alpha_3", "Name")], by=c("CNT"="Alpha_3")) %>%
  rename(name = Name)
 score_gap$name[score_gap$CNT == "KSV"] <- "Kosovo"
 }


library(forcats)
score_gap <- score_gap %>% 
  mutate(name = recode(name, "Czechia"="Czech Republic",
                       "Korea, Republic of"="South Korea",
                       "Macedonia, Republic of"="Macedonia",
                       "Moldova, Republic of"="Moldova",
                       "Russian Federation"="Russia",
                       "Taiwan, Province of China"="Taiwan",
                       "Trinidad and Tobago"="Trinidad",
                       "United States"="USA",
                       "United Kingdom"="UK",
                       "Viet Nam"="Vietnam")) 

glimpse(score_gap)
summary(score_gap)

ggplot(data=score_gap, aes(x=fct_reorder(name, math), y=math)) +
  geom_hline(yintercept=0, colour="red") +
  geom_point() + 
  geom_errorbar(aes(ymin=ml, ymax=mu), width=0) +
  coord_flip() + 
  xlab("") + ylab("Gender gap") + ylim(c(-35, 35))
```

  The math gender gap varies widely in between countries with an unweighted average gap of 4.8 points. The gender gap in maths appears to be wider in Europe and South America in favour of boys and wider in favour of girls in Middle East and the Caribbean. 
    

## Question 3

A cross-rate is *an exchange rate between two currencies computed by reference to a third currency, usually the US dollar.*
The data file `rates_Nov19_Mar20.csv` was extracted from https://openexchangerates.org [3] using the code below (my API key has been hidden from you):

```{r eval=FALSE}
# WARNING: YOU DON'T NEED TO RUN THIS CODE!!!!!
library(jsonlite)
library(lubridate)
library(tidyverse)
ru <- NULL
dt <- ymd("2019-11-01")
dt_end <- ymd("2020-03-31")
for (i in 1:151) {
  cat(i,"\n")
  url <- paste("http://openexchangerates.org/api/historical/",dt,".json?app_id=XXX", sep="")
  x <- fromJSON(url)
  x <- x$rates
  if (length(x) == 171)
    x <- x[-c(164,166)]
  ru <- rbind(ru, data.frame(date=dt, x))
  dt <- dt + days(1)
}
rownames(ru) <- ru$date
write_csv(ru, path="data/rates_new.csv")
```

a. What's the data? Make a plot of the Australian dollar against date. Explain how the Australian dollar has changed relative to the US dollar over the 5 month period.

  Over the 5 month period the Australian dollar has weakened against the US dollar with the currency remainig relatively stable in Q4 2019. The AUD begin a gradual decline at the start of 2020, this decline rapidly accelerated in early March, and reversed slightly in mid-late march. This period of high volatility coincides with when the coronavirus started affected the world economy to a greater extent.This may be due to interest rate cuts which encourage outflow of capital to places with higher potential returns like the US. There earlier decline in the value of the AUD has been caused by supression of demand due to the early impact of coronavirus in china. The bushfires also reduced australian output and supply of exports. [2] 

```{r}
library(tidyverse)
library(lubridate)
rates <- read_csv("data/rates_Nov19_Mar20.csv")
ggplot(rates, aes(x=ymd(date), y=AUD)) + 
  geom_line() + 
  ggtitle("USD:AUD Exchange Rates") +
  xlab("2019 - 2020 Month") +
  ylab("AUD per USD")
```

b. You are going to work with these currencies: AUD, CAD, CHF, CNY, EUR, GBP, INR, JPY, KRW, MXN, NZD, RUB, SEK, SGD, ZAR. List the names of the countries and currency name that these codes refer to. 

Secondary question: why is the USD a constant 1 in this data. 

*The US is the base rate, against which all other currencies are compared.*
    
  AUD - Australia - Australian Dollar
  
  CAD - Canada - Canadian Dollar
  
  CHF - Switzerland -Swiss Franc
  
  CNY - P.R. of China - RenMinBi
  
  EUR - European Union - Euro  
  
  GBP - United Kingdom - Pound Sterling
  
  INR - India - Indian Rupee 
  
  JPY - Japan - Japanese Yen
  
  KRW - S. Korea - Korean Won
  
  MXN - Mexico - Mexican Peso
  
  NZD - New Zealand - New Zealand Dollar 
  
  RUB - Russia - Russian Ruble
  
  SEK - Sweden -  Swedish Krona
  
  SGD - Singapore - Singapore Dollar
  
  ZAR - South Africa - South African Rand

c. The goal of the principal component analysis is to examine the relative movement of this subset of currencies, especially since coronavirus emerged until the end of March. PCA is used to summarise the volatility (variance) in the currencies, relative to each other. To do this you need to: 
  - Standardise all the currencies, individually. The resulting values will have a mean 0 and standard deviation equal to 1.
  - Flip the sign so that high means the currency strengthened against the USD, and low means that it weakened. Its easier to explain trends, if you don't need to talk with double-negatives.
  - Make a plot of all the currencies to check the result.

```{r}
#echo=TRUE, message=TRUE, warning=, paged.print=TRUE
library(viridisLite)
library(plotly)
library(lubridate)
rates_sub <- rates %>%
  select(c(date, USD, AUD, CAD, CHF, CNY, EUR, GBP, INR, JPY, KRW, MXN, NZD, RUB, SEK, SGD, ZAR)) %>%
  mutate_if(is.numeric, function(x) -1*(x-mean(x))/sd(x))
  rates_sub_long <- rates_sub %>% 
  pivot_longer(cols=-date, names_to="currency", values_to="std_r8s") 
  ggplot(rates_sub_long, aes(x=date, y=std_r8s, colour=currency)) + geom_line() +
  scale_colour_viridis_d("")
  ggplotly() #Make an interactive plot to browse the currencies
```

d. Conduct a principal component analysis on the subset of currencies. You need to work from a wide format of the data, where dates are in the columns, and currencies are in the rows. Normally, PCA operate on standardised variables but for this data, you need to NOT standardise each date. Think about why this is best.

  - Why is this data considered to be high-dimensional?

  Data sets containing more features than observations are often referred to as high-dimensional[1]. This data in wide form where the variables are the dates each of which comprise a dimension,  and each observation is a currency which results in n_dates >>> n_currencies. 
    
  - Make a scree plot to summarise the variance explained by cumulative principal components. How much of the total variation do two PCs explain?

The first 2 PCs explain roughly 55% of the variation in the data.
    
  - Plot the first two principal components. Write a summary of what you learn about the similarity and difference between the currencies.
      
![''](https://github.com/jlgao2/ETC3250/raw/master/images/a2q3d1.PNG)
      
  The majority of this basket appear to behave similarly as a group marked as cluster 1 on PC1 and PC2 with the notable exceptions of CNY, JPY, EUR and CHF. JPY and EUR are somewhat similar in PC1 and PC2, while the CNY and CHF stands somewhat alone. On PC1 the CHF and the JPY are somewhat similar in having a strong positive PC1 component. 

![''](https://github.com/jlgao2/ETC3250/raw/master/images/a2q3d2.PNG)

  - Plot the loadings for PC1. Add a base line set at $1/\sqrt{15}$. 
  - Why use this as a guide? 
  - What time frame generated a big movement (or divergence) in the currencies? 
  - Which currencies strengthened relative to the USD in that period? 
  - What happened to the Australian dollar? 
  - Answer these questions in a paragraph, written in your own words.
    
  The baselines are plotted on the loadings as the PCA formula states that loadings are $\displaystyle\sum_{j=1}^p \phi^2_{j1} = 1$, therefore $\pm \frac{1}{\sqrt150}$ represents the root mean square of the loadings as $p = 150$, this is used as a guide as it represents an 'average' of what the loadings should be for $p$ variables. There is a big movement in PC1 which started at around index 115 which corresponds to the end of February. In this period the EUR, JPY, CHF, and CNY strengthened relative to the US dollar, whereas the AUD, which is slightly negative in PC1 decrease in value relative to the USD during the same time period. 
    
  - Do the same analysis for PC2. What time frame was there another movement of currencies? Which currencies primarily strengthened, and which weakened during this period?

  For PC2 there was movement of currencies around index 70, and highly oscillatory behaviour starting around index 120. The outliers that are high in magnitude either direction of PC2 - CNY and CHF in the negative - increasing with negative loadings of PC2 vs EUR and JPY in the positive - increasing with positive loadings of PC2 shows inverse correlation between groups in those time periods.

  - Finish with a paragraph summarising what variability the principal components analysis is summarising. What dimension reduction is being done?
    
  The variability that PCA is summarising the variance between dates for each currency. We are reducing the date variables to a set of principle components. 
    
```{r}
library(ggrepel)
#library(ggfortify)
rates_sub_wide <- rates_sub_long %>%
  pivot_wider(id_cols=-date, names_from=date, values_from=std_r8s)
  rates_pca <- prcomp(rates_sub_wide[2:16,2:153], scale=FALSE)
screeplot(rates_pca, type="l")
  summary(rates_pca)
  rates_pca$x %>% 
  as_tibble() %>% 
  mutate("currency" = as_vector(rates_sub_wide[2:16,1])) %>%
  ggplot(aes(x=PC1, y=PC2)) + 
   geom_point() +
   geom_text(aes(x=PC1, y=PC2, label=currency)) + 
   theme(aspect.ratio=1)
rates_pc_loadings <- as_tibble(rates_pca$rotation) %>%
  mutate(date = colnames(rates_sub_wide[2:153]), 
         indx = 1:nrow(rates_pca$rotation),
         ymin=rep(0, nrow(rates_pca$rotation)))
ggplot(rates_pc_loadings) + 
  geom_hline(yintercept=c(1/sqrt(150),
                          -1/sqrt(150)), colour="red") + 
  geom_linerange(aes(x=indx, ymin=0, ymax=PC1)) +
  geom_point(aes(x=indx, y=PC1))
ggplot(rates_pc_loadings) + 
  geom_hline(yintercept=c(1/sqrt(150),
                          -1/sqrt(150)), colour="red") + 
  geom_linerange(aes(x=indx, ymin=0, ymax=PC2)) +
  geom_point(aes(x=indx, y=PC2))
```


## Question 4

What's wrong with the following statement?

**Principle component analysis is a dimension reduction technique**.

  Principle component analysis is both a dimension reduction technique but it is also a useful tool as applied here to understand the relationship between endogenous variables. We can use use the tool to more easily visualise higher dimension data. [1]

## Works Cited
[1]G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning: with Applications in R. New York: Springer-Verlag, 2013.

[2]Richardson. T, ‘Australian dollar plummets to 11-year low’, Australian Financial Review, Sydney, Mar. 02, 2020.

[3]Open Exchange Rates, Open Exchange Rates. JSON API, 2020.

[4]OECD, ‘2018 Database - PISA’, Organisation for Economic Co-operation and Development. https://www.oecd.org/pisa/data/2018database/#d.en.516012 (accessed Apr. 20, 2020).

