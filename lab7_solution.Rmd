---
title: "Lab 7 Solution"
subtitle: "Monash University, Econ & Bus Stat, ETC3250/5250"
author: "prepared by Professor Di Cook"
date: "Materials for Week 7"
output:
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  eval = TRUE,
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 6,
  fig.align = "center",
  cache = FALSE
)
```


# Class discussion 

The focus this week is understanding how to determine the importance of variables in tree and forest models. 

*Question 1: In the following tree,  what would you says is the order of importance of the variables for classifying region?*

```{r eval=TRUE}
library(tidyverse)
library(rpart)
library(rpart.plot)
olive <- read_csv("http://www.ggobi.org/book/data/olive.csv") %>%
  rename(name=X1) %>%
  dplyr::select(-name, -area) %>%
  mutate(region = factor(region))
olive_rp <- rpart(region~., data=olive)
prp(olive_rp)
```

**Top of the tree is most important.**

*Question 2: Read  the explanation of how variable importance is calculated from the original developers of the algorithm (Leo  Breiman and Adele Cutler), at https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm. Make a sketch to explain how this permutation approach can be used to measure variable importance.*

**Something like this below. If the permuted variable does worse than the actual variable  in  separating the classes, then the variable is considered  to be important. Not shown in this diagram is that only out-of-bag cases are used.**

![](images/variable_imp.jpg)

# Practice

1. Fit the tree to olive oils, using a training split of 2/3, using only regions 2, 3,  and  the predictors linoleic and arachidic. Report the training and test error, and make a plot of the boundary.

```{r out.width="100%", fig.width=8, eval=TRUE}
olive <- olive %>%
  filter(region != 1) %>%
  mutate(region = factor(region))  # need to reset levels
olive_sub <- olive %>%
  select(region, linoleic, arachidic)
library(caret)
set.seed(20200501)
tr_indx <- createDataPartition(olive_sub$region, times=10, p=0.67)
olive_tr <- olive_sub[tr_indx$Resample03,]
olive_ts <- olive_sub[-tr_indx$Resample03,]
olive_rp <- rpart(region~., data=olive_tr)
# Training error
1-confusionMatrix(olive_tr$region, predict(olive_rp, newdata=olive_tr, type="class"))$overall[1]
# Test error
1-confusionMatrix(olive_ts$region, predict(olive_rp, newdata=olive_ts, type="class"))$overall[1]
olive_grid <- expand_grid(linoleic = seq(500, 1500, 10), 
                          arachidic = seq(0, 105, 1))
olive_grid$region = predict(olive_rp, newdata=olive_grid, type="class")
ggplot() +
  geom_point(data=olive_grid, aes(x=linoleic, y=arachidic, colour=region), alpha=0.1) +
  geom_point(data=olive_sub, aes(x=linoleic, y=arachidic, colour=region, shape=region), size=2) +
  scale_colour_brewer("region", palette="Dark2") +
  theme(aspect.ratio=1)
```

2. Fit a random forest to the full data, using only linoleic and arachidic as predictors, report the out-of-bag error, and make a plot of the boundary.

```{r}
library(randomForest)
olive_rf <- randomForest(region~., data=olive_sub, importance=TRUE)
olive_rf
olive_grid <- expand_grid(linoleic = seq(500, 1500, 10), 
                          arachidic = seq(0, 105, 1))
olive_grid$region = predict(olive_rf, newdata=olive_grid, type="class")
ggplot() +
  geom_point(data=olive_grid, aes(x=linoleic, y=arachidic, colour=region), alpha=0.1) +
  geom_point(data=olive_sub, aes(x=linoleic, y=arachidic, colour=region, shape=region), size=2) +
  scale_colour_brewer("region", palette="Dark2") +
  theme(aspect.ratio=1)
```


3. Explain the difference between the single tree and random forest boundaries.

**The random forest makes a small l-shape in the boundary that allows it to better capture the difference  between groups. Its a very small change.**

4. Fit the random forest again to the full set of variables, and  compute the variable importance. Describe the order of importance of variables.

```{r}
olive_rf <- randomForest(region~., data=olive, importance=TRUE)
olive_rf
olive_rf$importance
```

5. Create a new variable called `linoarch` that is $0.377 \times linoleic + 0.926\times arachidic$. Make a plot of this variable against arachidic. Fit the tree model to the same training data using this variable in addition to linoleic and arachidic. Check the test error too. Why doesn't the tree use this new variable? It has a bigger difference between the two groups than linoleic?

```{r}
olive$linoarch <- 0.377*olive$linoleic + 0.926*olive$arachidic
olive_sub <- olive %>%
  select(region, linoleic, arachidic, linoarch)
ggplot() +
  geom_point(data=olive_sub, aes(x=linoarch, y=arachidic, colour=region, shape=region), size=2) +
  scale_colour_brewer("region", palette="Dark2") +
  theme(aspect.ratio=1)
olive_tr <- olive_sub[tr_indx$Resample03,]
olive_ts <- olive_sub[-tr_indx$Resample03,]
olive_rp <- rpart(region~., data=olive_tr) 
olive_rp
1-confusionMatrix(olive_ts$region, predict(olive_rp, newdata=olive_ts, type="class"))$overall[1]
olive_sub <- olive %>%
  select(region, arachidic, linoarch)
olive_tr <- olive_sub[tr_indx$Resample03,]
olive_ts <- olive_sub[-tr_indx$Resample03,]
olive_rp <- rpart(region~., data=olive_tr) 
olive_rp
1-confusionMatrix(olive_ts$region, predict(olive_rp, newdata=olive_ts, type="class"))$overall[1]
```

**The algorithm sees linoleic before it sees linoarch, and it uses this. To force the tree to ignore linoleic it needs to be removed. The test error for the tree based on linoarch is 0.**

6. Fit the random forest again to the full set of variables, including linoarch and  compute the variable importance. Describe the order of importance of variables. Does the forest see the new variable?

```{r}
olive_rf <- randomForest(region~., data=olive, importance=TRUE)
olive_rf
olive_rf$importance
```

**The variable importance shows that the forest can see linoarch as an important variable.**

7. This last question is see how random forests  can be  used in really tough problems. We use a forest to analyse the happy paintings by Bob Ross. This was the subject of the [538 post](http://fivethirtyeight.com/features/a-statistical-analysis-of-the-work-of-bob-ross/), "A Statistical Analysis of the Work of Bob Ross".

We have taken the painting images from the [sales site](http://www.saleoilpaintings.com/paintings/bob-ross/bob-ross-sale-3_1.html), read the images into R, and resized them all to be 20 by 20 pixels. Each painting has been classified into one of 8 classes based on the title of the painting. This is the data that you will work with.

It is provided in wide and long form. Long form is good for making pictures of the original painting, and the wide form is what you will need to use for fitting the classification models. In wide form, each row corresponds to one painting, and the rgb color values at each pixel are in each column. With a $20\times20$ image, this leads to $400\times3=1200$ columns.

Here are three of the original paintings in the collection, labelled as "scene", "water", "flowers":

![bobross5](images/bobross5.jpg)
![bobross140](images/bobross140.jpg)
![bobross167](images/bobross167.jpg)

```{r fig.width=8, fig.height=2.5}
library(gridExtra)
paintings <- read_csv("data/paintings-train.csv")
paintings_long <- read_csv("data/paintings-long-train.csv")
paintings_test <- read_csv("data/paintings-test.csv")
df <- filter(paintings_long, id == 5)
p1 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
df <- filter(paintings_long, id == 140)
p2 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
df <- filter(paintings_long, id == 167)
p3 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
grid.arrange(p1, p2, p3, ncol=3)
```


a. How  many  paintings  in the training data? Explain the difference between the long and the wide format of the data. What is the dimension of the data, used for the classification?
**178 paintings. The images are 20x20 which generates 400 variables but 3 colour channels gives 1200 variables. Long format is useful for plotting the painting as an image, but the forest needs wide form to do the fit.** 
b. Build a random forest for the training data, for two classes, `flowers` and `cold`. 
c. Predict the class of test set, report the error.**no error**
d. Which pixels are the most important for distinguishing these two types of paintings? 
**b317 is the variable that is most important. To decode this in terms of pixels, you need to look in the long for data, to row 317. It is pixel, (17,16	) and the blue component of colour.**

```{r results='hide', fig.show='hide'}
p_sub <- paintings %>% 
  filter(class %in% c("flowers", "cold")) %>% 
  arrange(class) %>%
  mutate(class = factor(class))
p_rf <- randomForest(class~., data=p_sub[,-c(1,2)], ntree=10000,
                     importance=TRUE)
p_rf

# Predict test
p_test_sub <- paintings_test %>% 
  filter(class %in% c("flowers", "cold")) %>% 
  arrange(class) %>%
  mutate(class = factor(class))

table(predict(p_rf, newdata=p_test_sub, type="class"), p_test_sub$class)

# Variable importance
p_rf_varimp <- p_rf$importance %>% 
  as_tibble() %>%
  mutate(var = rownames(p_rf$importance)) %>%
  arrange(desc(MeanDecreaseAccuracy))
p_rf_varimp %>% head()

```

<!--
# References

Cook, D. (2020) Introduction to Machine Learning slides https://iml.numbat.space

Baptiste Auguie (2017). gridExtra: Miscellaneous Functions for "Grid" Graphics.
  R package version 2.3. https://CRAN.R-project.org/package=gridExtra

Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani (2017). ISLR:
  Data for an Introduction to Statistical Learning with Applications in R. R
  package version 1.2. https://CRAN.R-project.org/package=ISLR
  
Max Kuhn (2020). caret: Classification and Regression Training. R package
  version 6.0-85. https://CRAN.R-project.org/package=caret

A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R
  News 2(3), 18--22.
  
Stephen Milborrow (2019). rpart.plot: Plot 'rpart' Models: An Enhanced Version
  of 'plot.rpart'. R package version 3.0.8.
  https://CRAN.R-project.org/package=rpart.plot
  
Terry Therneau and Beth Atkinson (2019). rpart: Recursive Partitioning and
  Regression Trees. R package version 4.1-15.
  https://CRAN.R-project.org/package=rpart
  
Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source
  Software, 4(43), 1686, https://doi.org/10.21105/joss.01686
-->