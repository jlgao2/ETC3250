---
title: "ETC3250/5250 Assignment 2"
date: SOLUTION
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE)
```


## Marks

- Total mark will be out or 25
- 3 points will be reserved for readability, and appropriate citing of external sources 
- 2 points will be reserved for reproducibility, that the report can be re-generated from the submitted Rmarkdown. 
- Accuracy and completeness of answers, and clarity of explanations will be the basis for the remaining 20 points. 

## Exercises

1. (5pts)This question is about the normal distribution, and how it relates to the classification rule provided by quadratic discriminant analysis.

    a. Write down the density function for a univariate normal distribution ($p=1$), with mean $\mu_k$ and variance $\sigma_k$. 
$$f_k(x) = \frac{1}{\sqrt{2 \pi} \sigma_k} \text{exp}~ \left\{ - \frac{1}{2 } \left(\frac{x - \mu_k}{\sigma_k}\right)^2 \right\}$$
    b. Show that the quadratic discriminant rule for two groups ($K=2$), $\pi_1=\pi_2$ is equal to:
*Assign a new observation $x_0$ to group 1 if*
$$-\frac12 \left( \frac{1}{\sigma_1^2}- \frac{1}{\sigma_2^2} \right) {x_0^2} + \left(\frac{\mu_1}{\sigma_1^2}-\frac{\mu_2}{\sigma_2^2}\right)x_0 -\frac12 \left(\frac{\mu_1^2}{\sigma_1^2}-  \frac{\mu_2^2}{\sigma_2^2} \right) + -\log{\sigma_1}+\log{\sigma_2}>0$$
*For* $\mu_1, \mu_2, \sigma_1, \sigma_2$ *and assume* $\pi_1=\pi_2$, 
\begin{align*}
&p_1(x_0) > p_2(x_0)\\
&\Rightarrow \frac{1}{\sqrt{2\pi}\sigma_1} \text{exp} \left\{ - \frac{1}{2 } \left(\frac{x_0 - \mu_1}{\sigma_1}\right)^2 \right\} > \frac{1}{\sqrt{2\pi}\sigma_2} \text{exp} \left\{ - \frac{1}{2 } \left(\frac{x_0 - \mu_2}{\sigma_2}\right)^2 \right\}\\
&\Rightarrow -\log \sigma_1 -\frac12\left(\frac{x_0-\mu_1}{\sigma_1}\right)^2 > -\log \sigma_2 -\frac12\left(\frac{x_0-\mu_2}{\sigma_2}\right)^2\\
&\Rightarrow -\log \sigma_1 -\frac{1}{2\sigma_1^2}\left(x_0^2-2\mu_1x_0+\mu_1^2\right) > -\log \sigma_2 -\frac{1}{2\sigma_2^2}\left(x_0^2-2\mu_2x_0+\mu_2^2\right) \\
&\Rightarrow -\frac12 \left( \frac{1}{\sigma_1^2}- \frac{1}{\sigma_2^2} \right) {x_0^2} + \left(\frac{\mu_1}{\sigma_1^2}-\frac{\mu_2}{\sigma_2^2}\right)x_0 -\frac12 \left(\frac{\mu_1^2}{\sigma_1^2}-  \frac{\mu_2^2}{\sigma_2^2} \right) - \log{\sigma_1}+\log{\sigma_2}> 0
\end{align*}
    c. Suppose $\mu_1=4, \mu_2=-5, \sigma_1=0.5, \sigma_2=5$ simulate a set of 50 observations from each population. Make a plot of the population model, and add these samples as a rug plot on the horizontal axis. (See the lecture notes for a similar plot and code for linear discriminant analysis.)
```{r fig.width=6, fig.height=4, out.width="80%"}
library(tidyverse)
set.seed("24042020")
n <- 100; n1 <- 50
m1 <- 4
m2 <- -5
s1 <- 0.5
s2 <- 5
x <- c(rnorm(n1, m1, s1), rnorm(n-n1, m2, s2))
y <- c(rep(1, n1), rep(2, n-n1))
df <- tibble(x, y)

x <- seq(-20, 10, 0.1)
dx <- c(dnorm(x, m1, s1), dnorm(x, m2, s2))
y <- factor(c(rep(1, length(x)), rep(2, length(x))))
df_pop <- tibble(x=c(x,x), dx, y)
p <- ggplot() + 
  geom_rug(data=df, aes(x=x, y=0, colour=factor(y)), alpha=0.7) +
  geom_line(data=df_pop, aes(x=x, y=dx, colour=y)) + 
  scale_color_brewer("", palette="Dark2") +
  xlab("x")  + ylab("density")
p
```
    d. Write down the rule using these parameter values, and sketch the boundary corresponding to the rule on the previous plot.
*Assign a new observation $x_0$ to group 1 if*
\begin{align*}
&-\frac12 \left( \frac{1}{\sigma_1^2}- \frac{1}{\sigma_2^2} \right) {x_0^2} + \left(\frac{\mu_1}{\sigma_1^2}-\frac{\mu_2}{\sigma_2^2}\right)x_0 -\frac12 \left(\frac{\mu_1^2}{\sigma_1^2}-  \frac{\mu_2^2}{\sigma_2^2} \right) + \log{\sigma_1}-\log{\sigma_2}>0\\
&\Rightarrow -\frac12 \left( \frac{1}{0.5^2}- \frac{1}{5^2} \right) {x_0^2} + \left(\frac{4}{0.5^2}-\frac{-5}{5^2}\right)x_0 -\frac12 \left(\frac{4^2}{0.5^2}-  \frac{(-5)^2}{5^2} \right) + \log{0.5}-\log{5}>0\\
&-1.9x_0^2 + 16.2x_0 -29.19 > 0
\end{align*}
*You can use a symbolic equation solver like https://www.symbolab.com/solver/quadratic-equation-calculator/, your quadratic equation solving skills, or rationalise that the boundary needs to be where the two normal densities intersect. This is at approximately 2.6 and 5.9.*

```{r}
p + geom_vline(xintercept=c(2.6, 5.9), linetype=2)
```
    e. If instead you had made a mistake and assumed that the two variances were equal, this would have produced a linear discrimant rule. Mark this boundary on the previous plot. Explain why and how this differs from result of the QDA rule.
*The boundary wuold be the average of the two means, that is -0.5. This doesn't provide a good boundary, because it doesn't take into account that group 1 has a much smaller variance than group 2. A lot of gruop 2 observations would be misclassified with the LDA rule.*
```{r}
p + geom_vline(xintercept=c(2.6, 5.9), linetype=2) +
  geom_vline(xintercept=-0.5, linetype=3, colour="red")
```

2. (4pts)In this question you are going to practice conducting bootstrap to obtain confidence intervals for a reasonably complicated yet simple analysis. 

*A significant gender gap in maths performance in favour of male students has returned, despite closing in 2015* [Natassia Chrysanthos, Sydney Morning Herald](https://www.smh.com.au/national/nsw/urgent-need-to-address-maths-performance-as-nsw-slumps-in-international-test-20191203-p53ge2.html)

Last December, the 2018 [OECD PISA results](http://www.oecd.org/pisa/data/) were released. These are standardised test scores in math, reading and science, of 15 year olds across the globe. It led to a flurry of articles in the news about slipping standards of Australian students. If you also browsed the news of other countries (including New Zealand, Indonesia, Finland), you would find that many had similarly woeful stories. The above headline focuses on the math gap. To explore this, we will compute bootstrap confidence intervals for the difference between weighted averages for boys and girls in each country. The data is from the 2015 results.  

The code block below will compute the difference between weighted averages for boys and girls in each country. A weighted average is often used with survey data, to reflect how the sampling was done relative to the population characteristics. The weighted average will typically better reflect the population mean. 

```{r gender_means}
library(tidyverse)
library(ISOcodes)
data("ISO_3166_1")

# Load data
load("data/pisa_scores.rda")

# The country information will be used to jooin the data with map data 
# and the ISOcodes package provides information about codes and country
scores <- scores %>% 
  mutate(CNT=recode(CNT, "QES"="ESP", "QCH"="CHN", "QAR"="ARG", "TAP"="TWN")) %>%
  filter(CNT != "QUC") %>%
  filter(CNT != "QUD") %>%
  filter(CNT != "QUE") %>%
  mutate(gender=factor(gender, levels=c(1,2), labels=c("female","male")))
score_gap <- scores %>% 
  group_by(CNT, gender) %>%
  summarise(math=weighted.mean(math, w=w, na.rm=T),
            reading=weighted.mean(reading, w=w, na.rm=T),
            science=weighted.mean(science, w=w, na.rm=T)) %>%
  pivot_longer(cols=math:science, names_to="test", values_to="score") %>%
  pivot_wider(names_from=gender, values_from=score) %>%
  mutate(gap = male - female) %>%
  pivot_wider(id_cols=CNT, names_from=test, values_from=gap)
```

This block of code will compute 90% bootstrap confidence intervals for the weighted mean difference. 

```{r}
library(boot)
# Compute confidence intervals
cifn <- function(d, i) {
  x <- d[i,]
  ci <- weighted.mean(x$math[x$gender=="male"], 
                                   w=x$w[x$gender=="male"], na.rm=T)-
                     weighted.mean(x$math[x$gender=="female"],
                                   w=x$w[x$gender=="female"], na.rm=T)
  ci
}
bootfn <- function(d) {
  r <- boot(d, statistic=cifn, R=100)
  l <- sort(r$t)[5]
  u <- sort(r$t)[95]
  ci <- c(l, u)
  return(ci)
}
#student2012.sub.summary.gap.boot <- ddply(student2012.sub, .(CNT), bootfn)
score_gap_boot <- scores %>% 
  split(.$CNT) %>% purrr::map(bootfn) %>% as_tibble() %>%
  pivot_longer(cols=ALB:VNM, names_to="CNT", values_to="value") %>%
  arrange(CNT) %>%
  mutate(bound=rep(c("ml","mu"), length(unique(scores$CNT)))) %>%
  pivot_wider(names_from = bound, values_from = value)
score_gap <- score_gap %>%
  left_join(score_gap_boot, by="CNT")
```

This block of code will add country names, and make dotplots with confidence intervals for the math gap for each country. 

```{r gap_dots, fig.height=8, fig.width=6, out.width="60%"}
score_gap <- score_gap %>%
  left_join(ISO_3166_1[,c("Alpha_3", "Name")], by=c("CNT"="Alpha_3")) %>%
  rename(name = Name)
score_gap$name[score_gap$CNT == "KSV"] <- "Kosovo"

library(forcats)
score_gap <- score_gap %>% 
  mutate(name = recode(name, "Czechia"="Czech Republic",
                       "Korea, Republic of"="South Korea",
                       "Macedonia, Republic of"="Macedonia",
                       "Moldova, Republic of"="Moldova",
                       "Russian Federation"="Russia",
                       "Taiwan, Province of China"="Taiwan",
                       "Trinidad and Tobago"="Trinidad",
                       "United States"="USA",
                       "United Kingdom"="UK",
                       "Viet Nam"="Vietnam")) 


ggplot(data=score_gap, aes(x=fct_reorder(name, math), y=math)) +
  geom_hline(yintercept=0, colour="red") +
  geom_point() + 
  geom_errorbar(aes(ymin=ml, ymax=mu), width=0) +
  coord_flip() + 
  xlab("") + ylab("Gender gap") + ylim(c(-35, 35))
```

Write a paragraph explaining what you learn about the math gap across the countries tested in 2015. 

*The gender gap for math is not universal. In a little more than half  of  the countries surveyed boys have a higher average math score than girls. In some countries girls scored higher on average than boys in math, and many countries have no significant difference between the averages.*

3. (9pts) A cross-rate is *an exchange rate between two currencies computed by reference to a third currency, usually the US dollar.*
The data file `rates_Nov19_Mar20.csv` was extracted from https://openexchangerates.org using the code below (my API key has been hidden from you):

```{r eval=FALSE}
library(jsonlite)
library(lubridate)
library(tidyverse)
ru <- NULL
dt <- ymd("2019-11-01")
dt_end <- ymd("2020-03-31")
for (i in 1:151) {
  cat(i,"\n")
  url <- paste("http://openexchangerates.org/api/historical/",dt,".json?app_id=XXX", sep="")
  x <- fromJSON(url)
  x <- x$rates
  if (length(x) == 171)
    x <- x[-c(164,166)]
  ru <- rbind(ru, data.frame(date=dt, x))
  dt <- dt + days(1)
}
rownames(ru) <- ru$date
write_csv(ru, path="data/rates_new.csv")

```

a. (1)What's the data? Make a plot of the Australian dollar against date. Explain how the Australian dollar has changed relative to the US dollar over the 5 month period.

*Over the 5 month period the Australian dollar has weakened against the US dollar, with a big  decline in mid-March as the coronavirus impact affected the world.*

```{r}
library(tidyverse)
rates <- read_csv("data/rates_Nov19_Mar20.csv")
ggplot(rates, aes(x=date, y=AUD)) + geom_line()
```

b. (1)You are going to work with these currencies: AUD, CAD, CHF, CNY, EUR, GBP, INR, JPY, KRW, MXN, NZD, RUB, SEK, SGD, ZAR. List the names of the countries and currency name that these codes refer to. Secondary question: why is the USD a constant 1 in this data. 

*The US is the base rate, against which all other currencies are compared.*

c. (2)The goal of the principal component analysis is to examine the relative movement of this subset of currencies, especially since coronavirus emerged until the end of March. PCA is used to summarise the volatility (variance) in the currencies, relative to each other. To do this you need to: 

    - Standardise all the currencies, individually. The resulting values will have a mean 0 and standard deviation equal to 1.
    - Flip the sigm so that high means the currency strengthened against the USD, and low means that it weakened. Its easier to explain trends, if you don't need to talk with double-negatives.
    - Make a plot of all the currencies to check the result.

```{r}
library(viridisLite)
library(plotly)
rates_sub <- rates %>%
  select(date, AUD, CAD, CHF, CNY, EUR, GBP, INR, JPY, KRW, MXN, NZD, RUB, SEK, SGD, ZAR) %>%
  mutate_if(is.numeric, function(x) -1*(x-mean(x))/sd(x))
rates_sub_long <- rates_sub %>% 
  pivot_longer(cols=AUD:ZAR, names_to="currency", values_to="crossrate") 
ggplot(rates_sub_long, aes(x=date, y=crossrate, colour=currency)) + geom_line() +
  scale_colour_viridis_d("")
# ggplotly() Make an interactive plot to browse the currencies
```

d. (5)Conduct a principal component analysis on the subset of currencies. You need to work from a wide format of the data, where dates are in the columns, and currencies are in the rows. Normally, PCA operate on standardised variables but for this data, you need to NOT standardise each date. Think about why this is best.

    - (0.5) Why is this data considered to be high-dimensional?
    - (1) Make a scree plot to summarise the variance explained by cumulative principal components. How much of the total variation do two PCs explain?
    - (1) Plot the first two principal components. Write a summary of what you learn about the similarity and difference between the curreencies. 
    - (1)Plot the loadings for PC1. Add a base line set at $1/\sqrt{15}$. Why use this as a guide? What time frame generated a big movement (or divergence) in the currencies? Which currencies strengthened relative to the USD in that period? What happened to the Australian dollar? Answer these questions in a paragraph, written in your own words.
    - (1) Do the same analysis for PC2. What time frame was there another movement of currencies? Which currencies primarily strengthened, and which weakened during this period?
    - (0.5) Finish with a paragraph summarising what variability the principal components analysis is summarising. What dimension reduction is being done?

    - *There are many more variables than observations, because we are considering the dates to be variables, and the currencies to be observations.*
    - *Two PCs explain 81% of the total variation.*
    - *Most of the currencies are reacting similarly because they are clumped together in the plot. The Chinese yuan, Swiss franc, Japanese yen and Euro are all behaving individually because each is something of an outlier in this plot.*
    - *The time period in March shows the greatest volatility in the currencies. The Euro, yen and france strengthened against the USD. The Australian dollar dropped in value substantially. This can be seen from time series plot, also.*
    - *From PC2 we can see that the Chinese yuan and Swiss franc are moving opposite to the Euro and yen. These correspond to various dates. In mid-January the yuan strengthened, and the yen declined. Early in the time period, November and December, the yuan and franc weakened. The Euro weakened in mid-February.*
    - *PCA is summarising the main patterns  of relative change in the currencies. It can be useful to get some rough understanding of the major fluctuations.*
    
```{r}
library(ggrepel)
rates_sub_wide <- rates_sub_long %>%
  pivot_wider(id_cols=currency, names_from=date, values_from = crossrate)
rates_pca <- prcomp(rates_sub_wide[,-1], scale=FALSE)
screeplot(rates_pca, type="l")
summary(rates_pca)
rates_pca$x %>% 
  as_tibble() %>% 
  mutate(currency = rates_sub_wide$currency) %>%
  ggplot(aes(x=PC1, y=PC2)) + 
    geom_point() +
    geom_text_repel(aes(x=PC1, y=PC2, label=currency)) + 
  theme(aspect.ratio=1)
rates_pc_loadings <- as_tibble(rates_pca$rotation[,1:2]) %>%
  mutate(date = rownames(rates_pca$rotation), 
         indx = 1:nrow(rates_pca$rotation),
         ymin=rep(0, nrow(rates_pca$rotation)))
ggplot(rates_pc_loadings) + 
  geom_hline(yintercept=c(-1/sqrt(nrow(rates_pca$rotation)),
                          1/sqrt(nrow(rates_pca$rotation))), colour="red") + 
  geom_errorbar(aes(x=indx, ymin=ymin, ymax=PC1)) +
  geom_point(aes(x=indx, y=PC1))
ggplot(rates_pc_loadings) + 
  geom_hline(yintercept=c(-1/sqrt(nrow(rates_pca$rotation)),
                          1/sqrt(nrow(rates_pca$rotation))), colour="red") + 
  geom_errorbar(aes(x=indx, ymin=ymin, ymax=PC2)) +
  geom_point(aes(x=indx, y=PC2))
```

4. (2pts)What's wrong with the following statement?

**Principle component analysis is a dimension reduction technique**.

*Principle has an entirely different meaning than principal. The latter means first or major, and the former means a convention or belief. The correct statement is: "Principal component analysis is a dimension reduction technqiue."*

