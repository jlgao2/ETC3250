img_rows <- 28
img_cols <- 28
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785])
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785])
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
View(y_train)
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785]) -1
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785]) -1
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
scores <- model %>% evaluate(
x_test, y_test, verbose = 0
)
# Output metrics
cat('Test loss:', scores[[1]], '\n')
cat('Test accuracy:', scores[[2]], '\n')```
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785]) -1
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785])
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
rm(list=ls())
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
# Load Libraries
rm(list=ls())
library(keras)
library(tidyverse)
library(caret)
# Load data
load("data/sketches_train.rda")
load("data/sketches_test.rda")
# Data Preparation
batch_size <- 128
num_classes <- 6
epochs <- 12
# Input image dimensions
img_rows <- 28
img_cols <- 28
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
x_train <- data.matrix(sketches[trainIndex, 1:784])
y_train <- data.matrix(sketches[trainIndex, 785]) -1
x_test  <- data.matrix(sketches[-trainIndex, 1:784])
y_test <- data.matrix(sketches[-trainIndex, 785])
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
View(y_train)
?fit
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
scores <- model %>% evaluate(
x_test, y_test, verbose = 0
)
View(y_train)
knitr::opts_chunk$set(echo = TRUE)
# Load Libraries
rm(list=ls())
library(keras)
library(tidyverse)
library(caret)
library(tidyr)
library(ggplot2)
# Load data
load("data/sketches_train.rda")
load("data/sketches_test.rda")
# Data Preparation
batch_size <- 128
num_classes <- 6
epochs <- 10
# Input image dimensions
img_rows <- 28
img_cols <- 28
class_names = c('banana',
'boomerang',
'cactus',
'crab',
'flip flops',
'kangaroo')
sketches <- sketches[sample(nrow(sketches)),]
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
train_images <- data.matrix(sketches[,1:784])
train_labels <- data.matrix(sketches[,785])-1
test_images <- data.matrix(sketches_test[, 1:784])
#test_labels <- data.matrix(sketches_test[, 785])-1
# Redefine  dimension of train/test inputs
train_images <- array_reshape(train_images, c(nrow(train_images), img_rows, img_cols, 1))
test_images <- array_reshape(test_images, c(nrow(test_images), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
train_labels[1:20]
# Transform greyscale values into [0,1] range
train_images <- train_images / 255
test_images <- test_images / 255
mea <- mean(train_images[,,,])
sds <- sd(train_images[,,,])
train_images <- (train_images - mea) / sds
test_images <- (test_images - mea) / sds
cat('train_images_shape:', dim(train_images), '\n')
cat(nrow(train_images), 'train samples\n')
cat(nrow(test_images), 'test samples\n')
# Convert class vectors to binary class matrices
train_labels <- to_categorical(train_labels, num_classes)
#test_labels <- to_categorical(test_labels, num_classes)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x=train_images,
y=train_labels,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
scores <- model %>% evaluate(
test_images, test_labels, verbose = 0
)
knitr::opts_chunk$set(echo = TRUE)
# Load Libraries
rm(list=ls())
library(keras)
library(tidyverse)
library(caret)
library(tidyr)
library(ggplot2)
# Load data
load("data/sketches_train.rda")
load("data/sketches_test.rda")
# Data Preparation
batch_size <- 128
num_classes <- 6
epochs <- 10
# Input image dimensions
img_rows <- 28
img_cols <- 28
class_names = c('banana',
'boomerang',
'cactus',
'crab',
'flip flops',
'kangaroo')
sketches <- sketches[sample(nrow(sketches)),]
trainIndex <- createDataPartition(sketches$word, times=1, list=F, p=0.8, groups=2)
train_images <- data.matrix(sketches[,1:784])
train_labels <- data.matrix(sketches[,785])-1
test_images <- data.matrix(sketches_test[, 1:784])
#test_labels <- data.matrix(sketches_test[, 785])-1
# Redefine  dimension of train/test inputs
train_images <- array_reshape(train_images, c(nrow(train_images), img_rows, img_cols, 1))
test_images <- array_reshape(test_images, c(nrow(test_images), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform greyscale values into [0,1] range
train_images <- train_images / 255
test_images <- test_images / 255
mea <- mean(train_images[,,,])
sds <- sd(train_images[,,,])
train_images <- (train_images - mea) / sds
test_images <- (test_images - mea) / sds
cat('train_images_shape:', dim(train_images), '\n')
cat(nrow(train_images), 'train samples\n')
cat(nrow(test_images), 'test samples\n')
# Convert class vectors to binary class matrices
train_labels <- to_categorical(train_labels, num_classes)
#test_labels <- to_categorical(test_labels, num_classes)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_sparse_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x=train_images,
y=train_labels,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x=train_images,
y=train_labels,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2,
shuffle = TRUE
)
# scores <- model %>% evaluate(
#   test_images, test_labels, verbose = 0
# )
# Output metrics
cat('Test loss:', scores[[1]], '\n')
# Model fitting -----------------------------------------------------------
# callbacks for weights and learning rate
input_img <- layer_input(shape = c(28, 28, 1))
model2 <- application_densenet(include_top = TRUE, input_tensor = input_img, dropout_rate = 0.2)
?application_densenet
# Model fitting -----------------------------------------------------------
# callbacks for weights and learning rate
input_img <- layer_input(shape = c(28, 28, 1))
model2 <- application_densenet(include_top = TRUE, input_tensor = input_img, classes = 6)
keras.applications
?keras.applications
??keras.applications
?densenet_preprocess_input
tensorflow.keras.applications.densenet
evaluate(model, x_test, y_test)
# Libraries ---------------------------------------------------------------
library(keras)
#' In this example we will train a DenseNet-40-12 to classify images from the
#' CIFAR10 small images dataset. This takes ~125s per epoch on a NVIDIA GEFORCE 1080 Ti,
#' so using a GPU is highly recommended.
#'
#' [DenseNet](https://arxiv.org/abs/1608.06993) is a network architecture where each
#' layer is directly connected to every other layer in a feed-forward fashion
#' (within each dense block). For each layer, the feature maps of all preceding
#' layers are treated as separate inputs whereas its own feature maps are passed on as
#' inputs to all subsequent layers. This connectivity pattern yields state-of-the-art
#' accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale
#' ILSVRC 2012 (ImageNet) dataset, DenseNet achieves a similar accuracy as ResNet, but using
#' less than half the amount of parameters and roughly half the number of FLOPs.
#'
#' Final accuracy on test set was 0.9351 versus 0.9300 reported on the
#' [paper](https://arxiv.org/abs/1608.06993).
#'
#' Beside the `keras` package, you will need to install the `densenet` package.
#' Installation instructions are available [here](https://github.com/dfalbel/densenet).
#'
# Libraries ---------------------------------------------------------------
library(keras)
library(densenet)
insta
install.packages("densenet")
install.packages("resnet")
install.packages(c("backports", "C50", "car", "carData", "dbplyr", "dplyr", "ggplot2", "haven", "httpuv", "keras", "modelr", "pkgload", "reticulate", "rmarkdown", "rversions", "sp", "tidyr", "tinytex", "xfun"))
knitr::opts_chunk$set(
echo = FALSE,
eval = TRUE,
message = FALSE,
warning = FALSE)
#import libraries
library(tidyverse)
library(wesanderson)
library(caret)
library(GGally)
library(tourr)
library(RColorBrewer)
library(plotly)
library(MASS)
library(rpart)
library(mltools)
library(psych)
knitr::opts_chunk$set(
echo = FALSE,
eval = TRUE,
message = FALSE,
warning = FALSE)
#includes
library(tidyverse)
library(ISLR)
library(xgboost)
library(caret)
library(mlr)
library(gbm)
library(knitr)
library(kableExtra)
library(e1071)
library(kernlab)
library(randomForest)
library(plotly)
library(GGally)
df <- tibble(id=1:15, x1=c(2, 3, 1, 4, 3, 4, 2, 5, 3, 1, 1, 2, 0, 1, 0),
x2=c(4, 2, 4, 4, 3, 3, 5, 3, 1, 3, 1, 1, 2, 2, 3),
class=c(rep(-1, 8), rep(1, 7)))
kable(df) %>% kable_styling()
p1 <- ggplot(data=df, mapping=aes(x=x1, y=x2, color=as_factor(class)))+
geom_point() +
geom_label(aes(label = id), nudge_x = 0.2)
p1
p2 <- p1 + geom_abline(aes(intercept = 4.5,  slope = -1))
p2
p3 <- p2 + geom_abline(aes(intercept = 4,  slope = -1),  linetype="dashed")+
geom_abline(aes(intercept = 5, slope = -1), linetype="dashed")
p3
p4 <- p3 +
geom_segment(mapping=aes(x = 1, xend = 1, y=4, yend=3.5), color="black", arrow=arrow()) +
geom_segment(mapping=aes(x = 1, xend = 1, y=3, yend=3.5), color="black", arrow=arrow()) +
geom_segment(mapping=aes(x = 3, xend = 3, y=1, yend=1.5), color="black", arrow=arrow()) +
geom_segment(mapping=aes(x = 3, xend = 3, y=2, yend=1.5), color="black", arrow=arrow())
p4
p5 <- p1 + geom_abline(aes(intercept = 4,  slope = -1),  linetype="dashed")+
geom_abline(aes(intercept = 5, slope = -1), linetype="dashed") +
geom_abline(aes(intercept = 4.25, slope = -0.9))
p5
df_mod <- add_row(df, id=16, x1=2, x2=2.5, class=1)
p6 <- ggplot(data=df_mod, mapping=aes(x=x1, y=x2, color=as_factor(class)))+
geom_point() +
geom_label(aes(label = id), nudge_x = 0.2)
p6
a = sqrt(1/403)
p7 = p6 + geom_abline(aes(intercept = 19/4,  slope = -1))
p7
mutate(df_mod, class=as_factor(class))
svm_1 <- svm(class ~ x1 + x2, data = df_mod, kernel="linear", scale = FALSE, type = "C-classification")
print(svm_1)
summary(svm_1)
cf <- coef(svm_1)
p8 <- p7 + geom_abline(mapping=aes(intercept=-cf[1]/cf[3], slope=-cf[2]/cf[3]), col = "red")
p8
library(tidyverse)
library(ISLR)
data(Caravan)
library(xgboost)
library(gbm)
library(randomForest)
library(caret)
Caravan %>% count(Purchase)
mycaravan <- Caravan %>%
mutate(Purchase = as.integer(ifelse(Caravan$Purchase == "Yes", 1, 0)))
set.seed(20200515)
tr_indx <- createDataPartition(mycaravan$Purchase, p=2/3)$Resample1
c_tr = mycaravan[tr_indx, ]
c_ts = mycaravan[-tr_indx, ]
c_rf <- randomForest(Purchase~., data=c_tr, ntree=500, importance=TRUE)
rf.prob <- predict(c_rf, newdata=c_ts)
rf.pred <- ifelse(rf.prob > 0.2, 1, 0)
addmargins(table(c_ts$Purchase, rf.pred))
c_rf <- randomForest(Purchase~., data=c_tr, ntree=500, importance=TRUE)
as_tibble(c_rf$importance) %>% bind_cols(var=rownames(c_rf$importance)) %>%
arrange(desc(IncNodePurity)) %>% print(n=6)
c_rf <- randomForest(Purchase~., data=c_tr, ntree=500, importance=TRUE)
library(tidyverse)
set.seed(20200515)
n <- 1000
b <- c(-2,-1)
x <- rnorm(n)
y <- b[1]+b[2]*x+rnorm(n)
df <- data.frame(x,y)
ggplot(df, aes(x, y)) + geom_point()
cbind(olive_svm$index, olive_svm$SV)
library(tidyverse)
library(e1071)
olive <- read_csv("http://www.ggobi.org/book/data/olive.csv") %>%
rename(name=X1) %>%
dplyr::select(-name, -area) %>%
filter(region != 1) %>%
mutate(region = factor(region))
olive_sub <- olive %>%
select(region, linoleic, arachidic)
library(caret)
set.seed(20200501)
tr_indx <- createDataPartition(olive_sub$region, times=10, p=0.67)
olive_tr <- olive_sub[tr_indx$Resample03,]
olive_ts <- olive_sub[-tr_indx$Resample03,]
olive_svm <- svm(region~linoleic+arachidic,  data=olive_tr, kernel="linear")
1-confusionMatrix(olive_tr$region, predict(olive_svm, newdata=olive_tr, type="class"))$overall[1]
1-confusionMatrix(olive_ts$region, predict(olive_svm, newdata=olive_ts, type="class"))$overall[1]
cbind(olive_svm$index, olive_svm$SV)
olive_svm$coefs
beta <- t(olive_svm$coefs)%*%olive_svm$SV
cbind(olive_svm$index, olive_svm$SV)
View(olive_svm)
olive_svm$SV
